[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My R4DS notes using webr",
    "section": "",
    "text": "Preface\nThis quarto book contains my notes of the book R for Data Science. I took this opportunity to learn webr and the live-htmlformat.\nTo learn more about Quarto books visit https://quarto.org/docs/books. More information on webr may be found at https://docs.r-wasm.org/webr/latest/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Visualization\nThis is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-transformation",
    "href": "01-intro.html#data-transformation",
    "title": "1  Introduction",
    "section": "1.2 Data transformation",
    "text": "1.2 Data transformation\n\n1.2.1 Rows\n\n\n\n\n\n\n\n\nHad an arrival delay of two or more hours:\n\n\n\n\n\n\n\n\nFlew to Houston (IAH or HOU):\n\n\n\n\n\n\n\n\nWere operated by United, American, or Delta:\n\n\n\n\n\n\n\n\nDeparted in summer (July, August, and September)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\n\n\n\n\n\n\n2.Sort flights to find the flights with longest departure delays. Find the flights that left earliest in the morning.\n\n\n\n\n\n\n\n\n3.Sort flights to find the fastest flights. (Hint: Try including a math calculation inside of your function.)\n\n\n\n\n\n\n\n\n4.Was there a flight on every day of 2013?\n\n\n\n\n\n\n\n\nYes!\n5.Which flights traveled the farthest distance? Which traveled the least distance?\n\n\n\n\n\n\n\n\n6.Does it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#columns",
    "href": "01-intro.html#columns",
    "title": "1  Introduction",
    "section": "1.3 Columns",
    "text": "1.3 Columns\n\n1.3.1 mutate()\nmutate(df, new_var = ...)\n\n\n1.3.2 select()\n\n\n\n\n\n\n\n\nThere are a number of helper functions you can use within select():\n\nstarts_with(\"abc\"): matches names that begin with “abc”.\nends_with(\"xyz\"): matches names that end with “xyz”.\ncontains(\"ijk\"): matches names that contain “ijk”.\nnum_range(\"x\", 1:3): matches x1, x2 and x3.\n\n\n\n1.3.3 rename()\nrename(df, new_var = old_var) select(df, new_var = old_var)\n\n\n1.3.4 relocate()\n\n\n1.3.5 Exercices\n1.Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?\n\n\n\n\n\n\n\n\n2.Brainstorm as many ways as possible to select dep_time, arr_time, and from {flights}.\n\n\n\n\n\n\n\n\n3.What happens if you specify the name of the same variable multiple times in a select() call?\n\n\n\n\n\n\n\n\n4.What does the any_of() function do? Why might it be helpful in conjunction with this vector?\n\n\n\n\n\n\n\n\n5.Does the result of running the following code surprise you? How do the select helpers deal with upper and lower case by default? How can you change that default?\n\n\n\n\n\n\n\n\nYes, it does surprise me since the variable names are lowercase but the string in contains() is uppercase. By default, contains() ignores case.\nTo change this default behavior, set ignore.case = FALSE.\n\n\n\n\n\n\n\n\n6.Rename air_time to air_time_min to indicate units of measurement and move it to the beginning of the data frame.\n\n\n\n\n\n\n\n\n7.Why doesn’t the following work, and what does the error mean?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#the-pipe",
    "href": "01-intro.html#the-pipe",
    "title": "1  Introduction",
    "section": "1.4 The pipe",
    "text": "1.4 The pipe",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#groups",
    "href": "01-intro.html#groups",
    "title": "1  Introduction",
    "section": "1.5 Groups",
    "text": "1.5 Groups\n\n1.5.1 group_by()\n\n\n1.5.2 summarize()\n\n\n1.5.3 The slice_functions\nThere are five handy functions that allow you extract specific rows within each group:\n\ndf |&gt; slice_head(n = 1) takes the first row from each group.\ndf |&gt; slice_tail(n = 1) takes the last row in each group.\ndf |&gt; slice_min(x, n = 1) takes the row with the smallest value of column x.\ndf |&gt; slice_max(x, n = 1) takes the row with the largest value of column x.\ndf |&gt; slice_sample(n = 1) takes one random row.\n\nYou can vary n to select more than one row, or instead of n =, you can use prop = 0.1 to select (e.g.) 10% of the rows in each group.\n[OBS] By default, slice_min() and slice_max() keep tied values so n = 1 means give us all rows with the highest value. If you want exactly one row per group you can set with_ties = FALSE.\n\n\n1.5.4 Grouping by multiple variables\nGroup of each date:\n\n\n\n\n\n\n\n\nWhen you summarize a tibble grouped by more than one variable, each summary peels off the last group. In hindsight, this wasn’t a great way to make this function work, but it’s difficult to change without breaking existing code. To make it obvious what’s happening, dplyr displays a message that tells you how you can change this behavior:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.5 Ungrouping with ungroup()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.6 .by argument for grouping\n\n\n\n\n\n\n\n\nOr if you want to group by multiple variables:\n\n\n\n\n\n\n\n\n[OBS.] .by works with all verbs and has the advantage that you don’t need to use the .groups argument to suppress the grouping message or ungroup() when you’re done.\n\n1.5.6.1 Exercises\n1.Which carrier has the worst average delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |&gt; group_by(carrier, dest) |&gt; summarize(n()))\n\n\n\n\n\n\n\n\n2.Find the flights that are most delayed upon departure from each destination.\n\n\n\n\n\n\n\n\n3.How do delays vary over the course of the day. Illustrate your answer with a plot.\n\n\n\n\n\n\n\n\n4.What happens if you supply a negative n to slice_min() and friends?\n\n\n\n\n\n\n\n\n5.Explain what count() does in terms of the dplyr verbs you just learned. What does the sort argument to count() do?\nIf sort = TRUE, the largest group is shown at the top.\n\n\n\n\n\n\n\n\n6.Suppose we have the following tiny data frame:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#case-study-aggregates-and-sample-size",
    "href": "01-intro.html#case-study-aggregates-and-sample-size",
    "title": "1  Introduction",
    "section": "1.6 Case study: aggregates and sample size",
    "text": "1.6 Case study: aggregates and sample size\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.6.1 Summary\nTo manipulate rows: filter(), arrarege()\nFor columns: select(), mutate(), group(), and summarize().",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#workflow-code-style",
    "href": "01-intro.html#workflow-code-style",
    "title": "1  Introduction",
    "section": "1.7 Workflow: code style",
    "text": "1.7 Workflow: code style\n\nnames\nspaces: Put spaces on either side of mathematical operators apart from ^ (i.e. +, -, ==, &lt;, …), and around the assignment operator (&lt;-).\nPipes: |&gt; should always have a space before it and should typically be the last thing on a line.\n\nIf the function you’re piping into has named arguments (like mutate() or summarize()), put each argument on a new line.\nIf the function doesn’t have named arguments (like select() or filter()), keep everything on one line unless it doesn’t fit, in which case you should put each argument on its own line.\nAfter the first step of the pipeline, indent each line by two spaces. RStudio will automatically put the spaces in for you after a line break following a |&gt; . If you’re putting each argument on its own line, indent by an extra two spaces. Make sure ) is on its own line, and un-indented to match the horizontal position of the function name.\nBe wary of writing very long pipes, say longer than 10-15 lines. Try to break them up into smaller sub-tasks, giving each task an informative name.\n\n\n\n\n\n\n\n\n\n\n4.The same basic rules that apply to the pipe also apply to {ggplot2}; just treat + the same way as |&gt;. Again, if you can’t fit all of the arguments to a function on to a single line, put each argument on its own line:\n\n\n\n\n\n\n\n\n5.Sectioning comments\n\n1.7.1 Exercises",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-tyding",
    "href": "01-intro.html#data-tyding",
    "title": "1  Introduction",
    "section": "1.8 Data tyding",
    "text": "1.8 Data tyding\n\n\n\n\n\n\n\n\n\n1.8.1 Exercises",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-import",
    "href": "01-intro.html#data-import",
    "title": "1  Introduction",
    "section": "1.9 Data import",
    "text": "1.9 Data import\nWe focus on importing CSV file. A CSV file looks like this: The first row, commonly called the header row, gives the column names, and the following six rows provide the data. The columns are separated, aka delimited, by commas.\n\n\n\n\n\n\n\n\nBy default, read_csv() only recognizes empty strings (\"\") in this dataset as NAs, we want it to also recognize the character string \"N/A\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn alternative approach is to use janitor::clean_names() to use some heuristics to turn them all into snake case at once:\n\n\n\n\n\n\n\n\nAnother common task after reading in data is to consider variable types. For example, meal_plan is a categorical variable, which in R should be represented as a factor:\n\n\n\n\n\n\n\n\n\n1.9.1 Other arguments\nUsually, read_csv() uses the first line of the data for the column names. If a few fist line include other text othen the columns names, you can use skip = n to skip the first n lines or use comment = \"#\" to drop all lines that start with (e.g.) #:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.9.2 Other file types\nOnce you’ve mastered read_csv(), it’s just a matter of knowing which function to reach for:\n\nread_csv2() reads semicolon-separated files. These use ; instead of , to separate fields and are common in countries that use , as the decimal marker.\nread_tsv() reads tab-delimited files.\nread_delim() reads in files with any delimiter, attempting to automatically guess the delimiter if you don’t specify it.\nread_fwf() reads fixed-width files. You can specify fields by their widths with fwf_widths() or by their positions with fwf_positions().\nread_table() reads a common variation of fixed-width files where columns are separated by white space.\nread_log() reads Apache-style log files.\n\n\n\n1.9.3 Exercises\n1\n2, 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#getting-help",
    "href": "01-intro.html#getting-help",
    "title": "1  Introduction",
    "section": "1.10 Getting help",
    "text": "1.10 Getting help",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#visualize",
    "href": "01-intro.html#visualize",
    "title": "1  Introduction",
    "section": "1.11 Visualize",
    "text": "1.11 Visualize\n\n1.11.0.1 Layers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can also set the visual properties of your geom manually as an argument of your geom function (outside of aes()) instead of relying on a variable mapping to determine the appearance\n\n\n\n\n\n\n\n\n\n\n1.11.1 Exercises\n1\n\n\n\n\n\n\n\n\n2\n3\nStroke aesthetic controls the size of the edge/border of the points for shapes 21-24 (filled circle, square, triangle, and diamond).\n\n\n\n\n\n\n\n\n\n\n1.11.2 Geometric objects\nNot every aesthetic works with every geom. You could set the shape of a point, but you couldn’t set the “shape” of a line.\n\n\n\n\n\n\n\n\nIf you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers. You can use the same idea to specify different data for each layer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.11.3 Exercises\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.11.3.1 Facets\n\n\n\n\n\n\n\n\nTo facet your plot with the combination of two variables, switch from facet_wrap() to facet_grid()\n\n\n\n\n\n\n\n\nBy default each of the facets share the same scale and range for x and y axes. This is useful when you want to compare data across facets but it can be limiting when you want to visualize the relationship within each facet better. To have different axis scales across both rows and columns, set the scales argument in a faceting function to \"free\":  \"free_x\" will allow for different scales across rows, and \"free_y\" will allow for different scales across columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "04-transform.html",
    "href": "04-transform.html",
    "title": "2  Transform",
    "section": "",
    "text": "2.1 Logical vectors",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "04-transform.html#numbers",
    "href": "04-transform.html#numbers",
    "title": "2  Transform",
    "section": "2.2 Numbers",
    "text": "2.2 Numbers",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "04-transform.html#strings",
    "href": "04-transform.html#strings",
    "title": "2  Transform",
    "section": "2.3 Strings",
    "text": "2.3 Strings\n\n2.3.1 Creating strings, escapes and raw strings\nUse ' or \" to create strings, \" is recommended per tidyverse style guide except when there are multiple quotes in a string. To include a literal single or double quote, use \\ to escape \\' and \\\". \\ is also used to escape special characters like new line \\n or tab \\t, \\\\ for backslash itself. See ?Quotes for all special characters.\n\ndouble_quote &lt;- \"\\\"\" # or '\"'\nsingle_quote &lt;- '\\'' # or \"'\"\nbackslash &lt;- \"\\\\\"\n\n\n\n\n\n\n\nNote\n\n\n\nThe printed representation of strings is different from the string itself. The printed string object shows the escapes. To see the row content of a string, use str_vew().\n\nx &lt;- c(single_quote, double_quote, backslash)\nx\n\n[1] \"'\"  \"\\\"\" \"\\\\\"\n\nstr_view(x)\n\n[1] │ '\n[2] │ \"\n[3] │ \\\n\n\n\n\nWhen you habve many escapes \\ in a string, so called leaning toothpick syndrome, you can use raw strings.\n\ntricky &lt;- \"double_quote &lt;- \\\"\\\\\\\"\\\" # or '\\\"'\nsingle_quote &lt;- '\\\\'' # or \\\"'\\\"\"\nstr_view(tricky)\n\n[1] │ double_quote &lt;- \"\\\"\" # or '\"'\n    │ single_quote &lt;- '\\'' # or \"'\"\n\n\n\ntricky_raw &lt;- r\"(double_quote &lt;- \"\\\"\" # or '\"'\nsingle_quote &lt;- '\\'' # or \"'\")\"\nstr_view(tricky_raw)\n\n[1] │ double_quote &lt;- \"\\\"\" # or '\"'\n    │ single_quote &lt;- '\\'' # or \"'\"\n\n\nA raw string starts with r\"( and ends with )\", and everything in between is taken literally, including quotes and backslashes. If the string )\", you can use r\"[ ]\", or r\"{ }\", or inserting any number of dashes to make the opening and closing delimiters unique, e.g. r\"--( )--\" or r\"---( )---\".\n\n\n\n\n\n\nNote\n\n\n\nstr_view() uses curly braces for tabs {\\t} to make them more visible.\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Exercises\n\nQuestion 1Solution\n\n\nCreate strings that contain the following values:\n\n  1. He said \"That's amazing!\"\n\n  2. \\a\\b\\c\\d\n\n  3. \\\\\\\\\\\\\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2SolutionHint\n\n\nCreate the string in your R session and print it. \nWhat happens to the special “\\u00a0”? \nHow does str_view() display it? \nCan you do a little googling to figure out what this special character is?\n\nx &lt;- \"This\\u00a0is\\u00a0tricky\"\n\n\n\n\n\n\n\n\n\n\n\n\n`\\u00a0` is a non-breaking space character.\n\n\n\n\n\n2.3.3 strings from data: str_c(), str_glue(), str_flatten()\n\n\n2.3.4 str_c()\nstr_c():\n\ntakes any number of vectors and returns a character vector similar to base::paste0().\n\n\n\n\n\n\n\n\n\n\nobeys tidyverse rule for recycling and propagating missing values NA: is designed to be used with mutate().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nif missing values should display in another way, use coalesce() inside or outside str_c().\n\n\n\n\n\n\n\n\n\n\n\n2.3.5 str_glue()\nstr_glue() from the {glue} package allows the use of {} to mix fixed and variable strings. This improves readability compared to str_c(). But str_glue() does not propagate missing values NA by default, so you may need to use coalesce().\nTo escape { or }, use double braces {{ or }}.\n\n\n\n\n\n\n\n\n\n\n2.3.6 str_flatten()\n\nstr_c() and str_glue() work well with mutate() because their output is the same length as their inputs.\nstr_flatten() takes a character vector, combines each element of the vector and returns a single string: it works well with summarize().\n\n\n\n\n\n\n\n\n\n\nstr_flatten()\nstr_flatten(\n  string, \n  collapse = \"\", \n  last = NULL, \n  na.rm = FALSE\n)\n\nstr_flatten_comma(\n  string,\n  last = NULL,\n  na.rm = FALSE\n)\n\nAn exemple of how str_flatten() works well with summarise():\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.7 Exercises\n\nQuestion 1Solution\n\n\nCompare and contrast the results of paste0() with str_c() for the following inputs:\n\nstr_c(\"hi \", NA)\nstr_c(letters[1:2], letters[1:3])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2SolutionHint\n\n\nWhat’s the difference between `paste()` and `paste0()?` How can you recreate the equivalent of `paste()` with `str_c()`?\n\n\n\n\n\n\n\n\n\n\n\n\npaste (..., sep = \" \", collapse = NULL, recycle0 = FALSE)\n\npaste0(...,            collapse = NULL, recycle0 = FALSE)\n\n\n\n\nQuestion 3Solution\n\n\nConvert the following expressions from str_c() to str_glue() or vice versa:\na. `str_c(\"The price of \", food, \" is \", price)`\nb. `str_glue(\"I'm {age} years old and live in {country}\")`\nc. `str_c(\"\\\\section{\", title, \"}\")`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.8 data from strings:\nIf multiple variables are crammed into a single string, you can use four tydr functions to separate them:\n\nseparate_longer_delim(col, delim) : creates new rows – delim splits string with delimiter e.g. \", \" or \" \"\nseparate_longer_position(col, width) : creates new rows – position splits string at specified widths e.g. c(3, 2)\nseparate_wider_delim(col, delim, names) : creates new columns\nseparate_wider_position(col, widths) : creates new columns\n\n\n[!Tips] Look at these too: str_split(), str_split_fixed(), str_extract(), and str_match().\n\n\n\n2.3.9 separate_longer_delim() and separate_longer_position()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.10 separate_wider_delim() and separate_wider_position()\nHere x object is made up of a code, an edition, and a year separated by \".\".\n\n\n\n\n\n\n\n\nIf a specific is not useful, you can use an NA to omit it from the results:\n\n\n\n\n\n\n\n\nseparate_wider_position() works a little differently: its arguments are the string, widths (of each column). widths = c(name=value, name=value, etc) a named integer vector, where the name gives the name of the new column, and the value is the number of characters it occupies. You can omit values from the output by not naming them:\n\n\n\n\n\n\n\n\n\n\n2.3.11 Diagnosing widening problems\nseparate_wider_delim() requires a fixed and known set of columns. If some rows don’t have the expected (equal) number of pieces, the function issues an error. To resolve the issue, the function has 2 arguments too_few and too_many.\n\n\n\n\n\n\n\n\nThe error gives even suggestion on how to proceed: debug or silence the error message.\n\n\n\n\n\n\n\n\nThe “debug” mode helps identify the input that succeeded (_ok is TRUE) and those that failed (_ok is FALSE). _pieces column shows how many pieces were found compared to the expected number of pieces (length of names argument). _remainderis more useful when there are too many pieces. _ok can be used to filter the rows that failed vs. succeeded.\n\n\n\n\n\n\n\n\nYou may want to fill the missing pieces with NA: too_few = align_start fills missing pieces at the end with NA, while too_few = align_end fills missing pieces at the start with NA.\n\n\n\n\n\n\n\n\nThe same principles apply when you have too many pieces: too_many = \"debug\" allows the input with extra/additional pieces in _remainder column.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can either ‘drop’ the extra pieces with too_many = \"drop\" or “merge” them into the last column with too_many = \"merge\".\n\n\n\n\n\n\n\n\n\n\n2.3.12 Letters\n\n\n\n\n\n\nNoteAims\n\n\n\n\nWork with individual letters in a string.\nFind the length of a string: str_length()\nextract substrings: str_sub()\nHandle long strings in plots and tables.\n\n\n\n\n\n2.3.13 str_length()\n\nstr_length() counts the number of characters (incl. spaces) in a string.\nUse with count() to find the distribtion of lengths of strings, e.g. baby names in the US.\nUse with filter() to find the shortest or longest strings/names.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.14 Subsetting with str_sub()\nstr_sub(string, start, end) extracts substrings from string, starting at position start and ending at position end. Negative values for start or end count backwards from the end of the string. The start and end arguments are inclusive so the length of the returned substring is end - start + 1.\n\n\n\n\n\n\n\n\nIf the string is too short, it returns as many characters as possible.\n\n\n\n\n\n\n\n\n\n\n2.3.15 Exercises\n\nQuestion 1Solution\n\n\nWhen computing the distribution of the length of babynames, why did we use wt = n?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nUse str_length() and str_sub() to extract the middle letter from each baby name. What will you do if the string has an even number of characters?\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3SolutionHint\n\n\nAre there any major trends in the length of babynames over time? What about the popularity of first and last letters?\n\n\nLength of babynames over time:\n\n\n\n\n\n\n\n\nPopularity of first letters over time:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.16 Non-English text\nRegarding non-English text, there are challenges one may face: endoding, letter variations, and locale-dependent functions.\n\ncharToRaw(\"Hadley\")\n\n[1] 48 61 64 6c 65 79\n\n\nEach of these hexadecimal numbers represents one letter. The mapping from hexadecimal numbers to letters is called an encoding. In this case the encoding is called ASCII. There are two different encoding in Europe, Latin1 and Latin2. But fortunately, today there is one standard that is supported almost everywhere: UTF-8 that can code about every character used by humans incl. symbols and emoji. {readr} uses UTF-8 everywhere. UTF-8 is good default but it will fail for data produced by older older systems. When this happens your strings will look weird.\n\n\n\n\n\n\n\n\nTo read this, specify the encoding with locale() argument of read_csv().\n\n\n\n\n\n\n\n\nTo find the encoding, use readr::guess_encoding() to help figure out. You may need to try a few before finding the right one.\nTo learn more on encoding, see https://kunststube.net/encoding/.\n\n\n2.3.17 Letter variations\nWorking with languages with accents is challenging as str_length() and str_sub() may not behave as expected.\n\n\n\n\n\n\n\n\nBut both strings differ in length and their first character:\n\n\n\n\n\n\n\n\nAlso, comparing these strings with == shows they are different, with str_equal() recognizes them as equal.\n\n\n\n\n\n\n\n\n\n\n2.3.18 Locale-dependent functions\nThere are some stringr functions that behave differently depending on the locale. A locale is specified by a lower-case language abbreviation, optionally followed by a _ and an upper-case region identifier, e.g. \"en_US\" for English as used in the United States, or \"fr_FR\" for French as used in France.\nTo see a list of supported locale in stringr, call stringi::stri_locale_list().\nBase R automatically uses the system locale. Base R string functions will do what you expect in your language but your code may behave differently if you share them to someone from another country. To avoid this stringr functions default to the \"en\" locale and require you to specify the locale to override it.\nThere are only two set of functions where the local matter: changing case and sorting.\nEx. Turkish has two different versions of the letter “i”: a dotted version “i” and a dotless version “ı”. Since they two different letters, they are capitalised differently:\n\n\n\n\n\n\n\n\nAnother example is sorting: sorting depends on the order of the alphabet which is not the same in every language. In Czech, “ch” is considered a single letter that comes after “h”.\n\n\n\n\n\n\n\n\ndplyr::arrange() has locale argument.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "04-transform.html#regex",
    "href": "04-transform.html#regex",
    "title": "2  Transform",
    "section": "2.4 Regex",
    "text": "2.4 Regex\n\n2.4.1 Introduction to regex\n\n\n\n\n\n\nNoteAims\n\n\n\n\nBasic regex and most useful stringr functions.\nSeven new topic: escaping, anchoring, character classes, shorthand classes, quantifiers, precedence, and grouping.\nFlags that allow to tweak the operation of regex.\nsurvey of other places in the tidyverse and base R where one might use regexes.\n\n\n\n\n\n\n\n\n\n\n\nThe chapter will use regex functions from stringr and tidyr as well as data from the babynames package, and three character vector from stringr: fruit (contains the names of 80 fruits), words (contains 900 common English words), and sentences (contains 720 short sentences).\n\n\n2.4.2 Pattern basics\nWe will use str_view() to see how regex patterns work. We will use it with its second argument, a regex.\nstr_view() will show only the elements of the string that match, surrounding it with &lt;&gt;.\n\n\n\n\n\n\n\n\nLetters and numbers match exactly and are called literal characters. Punctuation characters like ., (, ), \\, |, ?, *, +, {, }, ^, and $ have special meaning in regex and are called metacharacters:\n\n\n\n\n\n\nTipMetacharacters\n\n\n\n\n.: matches any character except a new line.\n^ and $: anchors to matches the start and end of a string.\n[]: defines character classes matches any character inside the brackets. ^ at the start of a character class negates it, matching any character not in the brackets.\n|: defines alternation, and means “or” (pick one or more alternative patterns).\n?, *, +, {}: quantifiers that specify how many times the preceding element should be matched`:\n\n? : matches 0 or 1 time (makes a pattern optional).\n* : matches 0 or more times.\n+ : matches 1 or more times.\n{} : matches a specific number of times.\n\n\n\n\nQuantifiers:\n\n\n\n\n\n\n\n\nCharacter classes:\n\n\n\n\n\n\n\n\nAlternation:\n\n\n\n\n\n\n\n\n\n\n2.4.3 str_detect() detects matches\nstr_detect(): - returns a logical vector that indicates whether or not each string contains a match to the pattern. - returns a logical vector of the same length as the input vector. - it pairs well with filter(). - it can be used with summerise()by pairing with sum() and mean(): sum(str_dectect(x, pattern)) to show the number of observations that match the pattern and mean(str_detect(x, pattern)) shows the proportion of observations that match the pattern.\n\n\n\n\n\n\n\n\nThis code finds all the most popular names containing a lower-case “x”:\n\n\n\n\n\n\n\n\nThe proportion of baby names4 that contain “x”, broken down by year:\n\n\n\n\n\n\n\n\nTwo other functions related to str_detect() are str_which() and str_subset(): str_which() return an integer vector giving the positions of the strings that match the pattern, while str_subset() returns a character vector containing only the strings that match the pattern.\n\n\n2.4.4 str_count() counts matches\nstr_count() tells how many matches there are in each string.\n\nx &lt;- c(\"apple\", \"banana\", \"pear\")\nstr_count(x, \"p\")\n\n[1] 2 0 1\n\n\nEach match starts at the end of the previous match, so regex matches never overlap.\n\nstr_count(\"abababa\", \"aba\")\n\n[1] 2\n\nstr_view(\"abababa\", \"aba\")\n\n[1] │ &lt;aba&gt;b&lt;aba&gt;\n\n\nstr_count() can be used with mutate(). In the exemple below, count the number of vowels and consonants in each name.\n\nbabynames |&gt;\n  count(name) |&gt;\n  mutate(\n    vowels = str_count(name, \"[aeiou]\"),\n    consonants = str_count(name, \"[^aeiou]\")\n  )\n\n# A tibble: 97,310 × 4\n   name          n vowels consonants\n   &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt;\n 1 Aaban        10      2          3\n 2 Aabha         5      2          3\n 3 Aabid         2      2          3\n 4 Aabir         1      2          3\n 5 Aabriella     5      4          5\n 6 Aada          1      2          2\n 7 Aadam        26      2          3\n 8 Aadan        11      2          3\n 9 Aadarsh      17      2          5\n10 Aaden        18      2          3\n# ℹ 97,300 more rows\n\n\nThe code found only 2 vowels in Aaban instead of 3 because regex are case-sensitive. To make them case-insensitive: - Add upper case to character classs: str_count(name, \"[aeiouAEIOU]\") - Use the argument ignore_case = TRUE: str_count(name, \"[aeiou]\", ignore_case = TRUE). - Use str_to_lower() or str_to_upper() to convert the strings to lower or upper case before counting: str_count(str_to_lower(name), \"[aeiou]\").\n\nbabynames |&gt;\n  count(name) |&gt;\n  mutate(\n    name = str_to_lower(name),\n    vowels = str_count(name, \"[aeiou]\"),\n    consonants = str_count(name, \"[^aeiou]\")\n  )\n\n\n\n2.4.5 str_replace() and str_replace_all() modifie matches\nstr_replace() replaces the first match, and str_replace_all() replaces all matches.\n\nx &lt;- c(\"apple\", \"pear\", \"banana\")\nstr_replace_all(x, \"[aeiou]\", \"-\")\n\n[1] \"-ppl-\"  \"p--r\"   \"b-n-n-\"\n\nstr_replace(x, \"[aeiou]\", \"-\")\n\n[1] \"-pple\"  \"p-ar\"   \"b-nana\"\n\n\nstr_remove() and str_remove_all() are shortcuts of str_replace(x, pattern, \"\") and str_replace_all(x, pattern, \"\").\n\n\n2.4.6 separate_wider_regex() extracts from columns into new columns\nThe family of separate_wider_*()functions live in tidyr because they operate on columns of data frames rather than on individual vectors.\nBelow are some data derived from babynames where we have the name, gender, and age of a bunch of people in a weird format:\n\ndf &lt;- tribble(\n  ~str              ,\n  \"&lt;Sheryl&gt;-F_34\"   ,\n  \"&lt;Kisha&gt;-F_45\"    ,\n  \"&lt;Brandon&gt;-N_33\"  ,\n  \"&lt;Sharon&gt;-F_38\"   ,\n  \"&lt;Penny&gt;-F_58\"    ,\n  \"&lt;Justin&gt;-M_41\"   ,\n  \"&lt;Patricia&gt;-F_84\" ,\n)\n\nUsing separate_wider_regex(), we can extract different parts of df$str into new columns:\n\ndf |&gt;\n  separate_wider_regex(\n    str,\n    pattern = c(\n      \"&lt;\",\n      name = \"[A-Za-z]+\",\n      \"&gt;-\",\n      gender = \"[NMF]\",\n      \"_\",\n      age = \"[0-9]+\"\n    )\n  )\n\n# A tibble: 7 × 3\n  name     gender age  \n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;\n1 Sheryl   F      34   \n2 Kisha    F      45   \n3 Brandon  N      33   \n4 Sharon   F      38   \n5 Penny    F      58   \n6 Justin   M      41   \n7 Patricia F      84   \n\n\nIf the match fails, use too_few = debug to diagnose the problem.\n\n\n2.4.7 Exercises\n\nQuestion 1SolutionHint\n\n\nWhat baby name has the most vowels? What name has the highest proportion of vowels? (Hint: what is the denominator?)\n\n\n\n\n\n\n\n\n\n\n\n\nbabynames |&gt;\n  count(name) |&gt; \n  mutate(\n    name = str_to_lower(name),\n    n_vowels = str_count(name, \"[ aeiou ]\"),\n    total_letters = str_count(name, \"[a-z]\"),\n    prop_vowel = n_vowels/total_letters\n  ) |&gt; \n1    arrange(desc(n_vowels)) |&gt;\n2    arrange(desc(prop_vowel))\n\n1\n\nShow most vowels at the top: 15\n\n2\n\nShow highest proportion of vowels at the top: 0.85 (after 1 those with only vowels)\n\n\n\n\n\n\nQuestion 2SolutionHint\n\n\nReplace all forward slashes in “a/b/c/d/e” with backslashes. What happens if you attempt to undo the transformation by replacing all backslashes with forward slashes? (We’ll discuss the problem very soon.)\n\n\n\n\n\n\n\n\n\n\n\n\nIt seems like, slash in the pattern argument needs to be escaped with \\\\, but replacement argument we do not need to escape.\n\n\n\n\nQuestion 3SolutionHint\n\n\nImplement a simple version of str_to_lower() using str_replace_all().\n\n\n\n\n\n\n\n\n\n\n\n\nbabynames |&gt; \n  count(name) |&gt; \n  mutate(\n   vers = str_replace_all(name, \"^[A-Z]?\", \"\")\n  )\n\n\n\n\nQuestion 4SolutionHint\n\n\nCreate a regular expression that will match telephone numbers as commonly written in your country.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"^(\\+46|0)(\\s?-?\\(?\\d{1,3}\\)?)([\\s-]?\\d{2,3}){2,3}$\"\n\n1^(\\+46|0)\n2(\\s?-?\\(?\\d{1,3}\\)?)\n3([\\s-]?\\d{2,3}){2,3}$\n\n1\n\nStart of string ^ with +46 or 0046 or 0. Obs. \\+ to escape + and | alternation.\n\n2\n\n\\s? matches any whitespace (space, tab); ? makes it optional. -? optional dash. \\(? literal ( that is optional. \\d matches a 0-9 digit; quantifier {1,3} specifies 1 to 3 digits. \\)? optional ).\n\n3\n\n([\\s-]?\\d{2,3}){2,3}$ matches the second part of the phone number, which is 2 to 3 digits, and is repeated 2 to 3 times. Meaning that it matches 2-3 groups of 2-3 preceded by an optional space or dash. $ asserts position at the end of the string.\n\n\n() specify capturing groups. Regex can match even without () to only get the full match with str_match(). But with () str_match() returns a matrix where the first column is the complete match, and subsequent columns are the capturing groups.\n\n\n\n\n\n2.4.8 Pattern details\n\n\n2.4.9 Escaping metacharacters\nBecause \\ is used as an escape symbol in strings, you need to use double backslashes \\\\ to escape metacharacters in regex patterns.\n\ndot &lt;- \"\\\\.\"\n\nstr_view(dot)\n\n[1] │ \\.\n\nstr_view(c(\"abc\", \"a.c\", \"bef\"), \"a\\\\.c\")\n\n[2] │ &lt;a.c&gt;\n\n\nTo match a literal backslash \\, you need to escape it creating regex \\\\. To create this regex, you need to use a string which also needs to escape each backslash, resulting in four backslashes \\\\\\\\ to match a single backslash.\n\n\n\n\n\n\nImportant\n\n\n\nIn this book, we’ll usually write regular expression without quotes, like \\.. If we need to emphasize what you’ll actually type, we’ll surround it with quotes and add extra escapes, like \"\\\\.\".\n\n\n\nx &lt;- \"a\\\\b\"\nstr_view(x)\n\n[1] │ a\\b\n\nstr_view(x, \"\\\\\\\\\")\n\n[1] │ a&lt;\\&gt;b\n\n\nRaw strings can be easier to use:\n\nstr_view(x, r\"{\\\\}\")\n\n[1] │ a&lt;\\&gt;b\n\n\nAnother alternative when trying to literal ., $, *, |, +, ?, (, ), {, } is to use a character class: [.], [$], [|], etc.\n\nstr_view(c(\"abc\", \"a.c\", \"a*c\", \"a c\"), \"a[.]c\")\n\n[2] │ &lt;a.c&gt;\n\nstr_view(c(\"abc\", \"a.c\", \"a*c\", \"a c\"), \".[*]c\")\n\n[3] │ &lt;a*c&gt;\n\n\n\n\n2.4.10 Anchors: ^ and $\n\nhead(fruit)\n\n[1] \"apple\"       \"apricot\"     \"avocado\"     \"banana\"      \"bell pepper\"\n[6] \"bilberry\"   \n\nstr_view(fruit, \"^a\")\n\n[1] │ &lt;a&gt;pple\n[2] │ &lt;a&gt;pricot\n[3] │ &lt;a&gt;vocado\n\nstr_view(fruit, \"a$\")\n\n [4] │ banan&lt;a&gt;\n[15] │ cherimoy&lt;a&gt;\n[30] │ feijo&lt;a&gt;\n[36] │ guav&lt;a&gt;\n[56] │ papay&lt;a&gt;\n[74] │ satsum&lt;a&gt;\n\n\nYou can match the boundary between words with \\b. \\bsum\\b matches sum() but not summary() or summarize().\n\nx &lt;- c(\"summary(x)\", \"summarize(df)\", \"rowsum(x)\", \"sum(x)\")\nstr_view(x, \"sum\")\n\n[1] │ &lt;sum&gt;mary(x)\n[2] │ &lt;sum&gt;marize(df)\n[3] │ row&lt;sum&gt;(x)\n[4] │ &lt;sum&gt;(x)\n\nstr_view(x, \"\\\\bsum\\\\b\")\n\n[4] │ &lt;sum&gt;(x)\n\n\nWhen used alone, anchors will produce a zero-width match:\n\nstr_view(\"abc\", c(\"$\", \"^\", \"\\\\b\"))\n\n[1] │ abc&lt;&gt;\n[2] │ &lt;&gt;abc\n[3] │ &lt;&gt;abc&lt;&gt;\n\n\nSee what happens when you replace a standalone anchor:\n\nstr_replace_all(\"abc\", c(\"$\", \"^\", \"\\\\b\"), \"--\")\n\n[1] \"abc--\"   \"--abc\"   \"--abc--\"\n\n\n\n\n2.4.11 Character classes\nCharacter classs or character set allows to match any character from a set of characters defined in [].\n\n\n\n\n\n\nTipCharacter classes\n\n\n\n\n[abc] matches any of the letters a, b, or c,\n[^abc] matches any character except a, b, or c\n- defines a range such as [a-z] matches any lower-case letter and [0-9] matches any digit.\n\\ escapes special characters, such as [\\^\\-\\]] matches a caret ^, hyphen -, or closing bracket ].\n\\d matches a digit (equivalent to [0-9]),\n\\D matches anything that is not a digit,\n\\w matches a word character (equivalent to [A-Za-z0-9_] - matches all excluding space, punctuation, symbols and special characters),\n\\W matches anything that is not a word character,\n\\s matches a whitespace character (space, tab, newline).\n\\S matches anything that is not a whitespace.\n\n\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\n2.4.11.1 Hint\nx &lt;- \"abcd ABCD 12345 -!@#%.\"\n1str_view(x, \"\\\\d+\")\n2str_view(x, \"\\\\D+\")\n3str_view(x, \"\\\\s+\")\n4str_view(x, \"\\\\S+\")\n5str_view(x, \"\\\\w+\")\n6str_view(x, \"\\\\W+\")\n\n1\n\nmatches one or more digits\n\n2\n\nmatches all that is not digits, one or more times\n\n3\n\nmatches one or more whitespace characters\n\n4\n\nmatches all that is not whitespace, one or more times\n\n5\n\nmatches one or more word characters\n\n6\n\nmatches all that is not word characters, one or more times, such as space, newline, tab, punctuation, symbols and special characters.\n\n\n\n\n\n\n\n\n2.4.12 Quantifiers: ?, *, +, {}\nQuantifiers control how many times a pattern matches.\n\n\n\n\n\n\nTipQuantifiers\n\n\n\n\n? matches 0 or 1 time (makes a pattern optional).\n* matches 0 or more times.\n+ matches 1 or more times.\n{n} matches exactly n times.\n{n,} matches n or more times.\n{n,m} matches between n and m times.\n\n\n\n\n\n2.4.13 Operator precedence and ()\nIn regex, quantifiers have higher precedence and alternation has low precedence: ab+ is equal to a(b+), while ab|cd is equal to (ab)|(cd). Feel free to use parentheses () to remeber the precedence rule and override operator precedence for regex.\n\n\n2.4.14 Grouping and capturing with ()\nParentheses () allow also to create capturing groups that allow to use sub-components of the match.\nThe first way to use capturing group is to refer back to it within a match with back reference: \\1, \\2, etc. refer to the first, second, etc. parentheses (capturing group).\n1str_view(fruit, \"(..)\\\\1\")\n#&gt; [4] │ b&lt;anan&gt;a          \n#&gt;[20] │ &lt;coco&gt;nut         \n#&gt;[22] │ &lt;cucu&gt;mber        \n#&gt;[41] │ &lt;juju&gt;be          \n#&gt;[56] │ &lt;papa&gt;ya          \n#&gt;[73] │ s&lt;alal&gt; berry      \n\n1\n\n(..) captures any two characters; and \\1 refers to the first capturing group, so the pattern matches any two characters followed by the same two characters.\n\n\nThe code below find any word with has same 2 letter at the beginning with ^(..) and the end of the word by capturing the first back reference \\\\1$ . .* specifies any characters in between.\n\n\n\n\n\n\n\n\nYou can use back references in str_replace(). The code below switches the order of the second and third words in sentences:\n\n\n\n\n\n\n\n\nThe matches can be extracted with str_match() with the full match and the specific matches; but this returns a matrix, which is not easy to work with. The matrix can be converted to a tibble and name the columns:\n\n\n\n\n\n\n\n\nThis is how separate_wider_regex()works behind the scenes. It converts the vector of patterns to a single regex that uses grouping to capture the named components.\nWhen you want to use () without creating matching groups, use non-capturing groups (?:).\n\n\n\n\n\n\n\n\n\n\n2.4.15 Exercises\n\nQuestion 1SolutionHint\n\n\nHow would you match literal string \"'\\\"? How about \"$^$\"?\n\n\nBy escaping ’ and . “\\’” and “\\\\”\n\n\n\n\n\n\n\n\n\n\n1x &lt;- c(\"'\\\\\", \"$^$\")\n\n2str_view(x, \"(\\\\'\\\\\\\\)|(\\\\$\\\\^\\\\$)\")\n\n1\n\nTo create \\ in a string, it needs to be escaped with another \\ as it is used to escape other characters. ' does not need to be escaped within \"\".\n\n2\n\n\\\\' matches literal ' and \\\\\\\\ matches literal \\. As string ' can be created with \"'\" without escaping, even \\' and 'without escaping matches literaral '.\n\n\n\n\n\n\nQuestion 2SolutionHint\n\n\nExplain why each of these patterns don’t match a \\: \"\\\", \"\\\\\", \"\\\\\\\".\n\n\n\n\n\n\n\n\n\n\n\n\nSee Hint under Question 1 of Exercises 2.4.15.\n\n\n\n\nQuestion 3SolutionHint\n\n\nGiven the corpus of common words in stringr::words, create regular expressions that find all words that:\n\nStart with “y”.\nDon’t start with “y”.\nEnd with “x”.\nAre exactly three letters long. (Don’t cheat by using str_length()!)\nHave seven letters or more.\nContain a vowel-consonant pair.\nContain at least two vowel-consonant pairs in a row.\nOnly consist of repeated vowel-consonant pairs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4SolutionHint\n\n\ncreate 11 regular expressions that match the british or american spellings for each of the following words: airplane/aeroplane, aluminum/aluminium, analog/analogue, ass/arse, center/centre, defense/defence, donut/doughnut, gray/grey, modeling/modelling, skeptic/sceptic, summarize/summarise. try and make the shortest possible regex!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5Solution\n\n\nSwitch the first and last letters in words. Which of those strings are still words?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6SolutionHint\n\n\nDescribe in words what these regular expressions match: (read carefully to see if each entry is a regular expression or a string that defines a regular expression.)\n\n^.*$\n\n\"\\\\{.+\\\\}\"\n\\d{4}-\\d{2}-\\d{2}\n\"\\\\\\\\{4}\"\n\\&lt;\\.\\.\\.\\&gt;\n(.)\\1\\1\n\"(..)\\\\1\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmatches any string.\nmatches any sting of 1 or more letters enclosed in {}\nmatches any string of digits composed as a date 4 digits, 2 digits, and 2 digits separated with a dash. looks like a date format yyy-mm-dd.\nmatches a string of four backslashes.\nmatches a string of three points enclosed in &lt; &gt;.\nmatches any character repeated three times in a row.\nmatches any two characters repeated twice in a row.\n\n\n\n\n\n2.4.15.1 Question 7\nSolve the beginner regexp crosswords at https://regexcrossword.com/challenges/beginner.\n\n\n\n2.4.16 Pattern control\nFlags can be used to control the default behavior of regex. In stringr, flags can be used by wrapping the pattern in a call to regex(). Some useful flags are:\n\nignore_case = TRUE: macth both upper- and lowercase\n\n\nbananas &lt;- c(\"banana\", \"Banana\", \"BANANA\")\nstr_view(bananas, \"banana\")\n\n[1] │ &lt;banana&gt;\n\nstr_view(bananas, regex(\"banana\", ignore_case = TRUE))\n\n[1] │ &lt;banana&gt;\n[2] │ &lt;Banana&gt;\n[3] │ &lt;BANANA&gt;\n\n\n\ndotall = TRUE: lets . match everything, incl. \\n.\n\n\nx &lt;- \"Line 1\\nLine 2\\nLine 3\"\nstr_view(x, \".Line\")\n\nstr_view(x, regex(\".Line\", dotall = TRUE))\n\n[1] │ Line 1&lt;\n    │ Line&gt; 2&lt;\n    │ Line&gt; 3\n\n\n\nmultiline = TRUE: makes ^ and $ match the start and end of each line rather than the start/end of a string.\n\n\nx &lt;- \"Line 1\\nLine 2\\nLine 3\"\nstr_view(x, \"^Line\")\n\n[1] │ &lt;Line&gt; 1\n    │ Line 2\n    │ Line 3\n\nstr_view(x, regex(\"^Line\", multiline = TRUE))\n\n[1] │ &lt;Line&gt; 1\n    │ &lt;Line&gt; 2\n    │ &lt;Line&gt; 3\n\n\n\ncomments = TRUE: tweaks the pattern language to ignore everything after #.\n\n\nphone &lt;- regex(\n  r\"(\n    \\(?     # optional opening parens\n    (\\d{3}) # area code\n    [)\\-]?  # optional closing parens or dash\n    \\ ?     # optional space\n    (\\d{3}) # another three numbers\n    [\\ -]?  # optional space or dash\n    (\\d{4}) # four more numbers\n  )\", \n  comments = TRUE\n)\n\nstr_extract(c(\"514-791-8141\", \"(123) 456 7890\", \"123456\"), phone)\n\n[1] \"514-791-8141\"   \"(123) 456 7890\" NA              \n\n\nIf you are using comments, to have a space, newlune or #, we need to escape it with \\.\n\n\n2.4.17 Fixed matches\nfixed() can be used instead of regex().\n\nstr_view(c(\"\", \"a\", \".\"), fixed(\".\"))\n\n[3] │ &lt;.&gt;\n\nstr_view(\"x X\", \"X\")\n\n[1] │ x &lt;X&gt;\n\nstr_view(\"x X\", fixed(\"X\", ignore_case = TRUE))\n\n[1] │ &lt;x&gt; &lt;X&gt;\n\n\nIf you are working with non-English languages, use coll() instead of fixed() as it impliments the full rules for capitalization and locale.\n\nstr_view(\"i İ ı I\", fixed(\"İ\", ignore_case = TRUE))\n\n[1] │ i &lt;İ&gt; ı I\n\nstr_view(\"i İ ı I\", coll(\"İ\", ignore_case = TRUE, locale = \"tr\"))\n\n[1] │ &lt;i&gt; &lt;İ&gt; ı I\n\n\n\n\n2.4.18 Practice\n\nChecking your work\n\n\nlibrary(tidyverse)\nlibrary(babynames)\n\n# let’s find all sentences that start with “The”. \nstr_view(head(sentences), \"^The\")\n\n[1] │ &lt;The&gt; birch canoe slid on the smooth planks.\n[4] │ &lt;The&gt;se days a chicken leg is a rare dish.\n[6] │ &lt;The&gt; juice of lemons makes fine punch.\n\n# We need to make sure that the “e” is the last letter in the word, which we can do by adding a word boundary:\nstr_view(sentences, \"^The\\\\b\" )\n\n [1] │ &lt;The&gt; birch canoe slid on the smooth planks.\n [6] │ &lt;The&gt; juice of lemons makes fine punch.\n [7] │ &lt;The&gt; box was thrown beside the parked truck.\n [8] │ &lt;The&gt; hogs were fed chopped corn and garbage.\n[11] │ &lt;The&gt; boy was there when the sun rose.\n[13] │ &lt;The&gt; source of the huge river is the clear spring.\n[18] │ &lt;The&gt; soft cushion broke the man's fall.\n[19] │ &lt;The&gt; salt breeze came across from the sea.\n[20] │ &lt;The&gt; girl at the booth sold fifty bonds.\n[21] │ &lt;The&gt; small pup gnawed a hole in the sock.\n[22] │ &lt;The&gt; fish twisted and turned on the bent hook.\n[24] │ &lt;The&gt; swan dive was far short of perfect.\n[25] │ &lt;The&gt; beauty of the view stunned the young boy.\n[28] │ &lt;The&gt; colt reared and threw the tall rider.\n[36] │ &lt;The&gt; wrist was badly strained and hung limp.\n[37] │ &lt;The&gt; stray cat gave birth to kittens.\n[38] │ &lt;The&gt; young girl gave no clear response.\n[39] │ &lt;The&gt; meal was cooked before the bell rang.\n[42] │ &lt;The&gt; ship was torn apart on the sharp reef.\n[44] │ &lt;The&gt; wide road shimmered in the hot sun.\n... and 236 more\n\n# What about finding all sentences that begin with a pronoun?\nstr_view(sentences, \"^(She\\\\b|He\\\\b|We\\\\b|I\\\\b|You\\\\b|They\\\\b)\")\n\n [61] │ &lt;We&gt; talked of the side show in the circus.\n [63] │ &lt;He&gt; ran half way to the hardware store.\n [90] │ &lt;He&gt; lay prone and hardly moved a limb.\n[116] │ &lt;He&gt; ordered peach pie with ice cream.\n[120] │ &lt;We&gt; find joy in the simplest things.\n[132] │ &lt;He&gt; said the same phrase thirty times.\n[139] │ &lt;We&gt; frown when events take a bad turn.\n[153] │ &lt;He&gt; broke a new shoelace that day.\n[158] │ &lt;We&gt; tried to replace the coin but failed.\n[159] │ &lt;She&gt; sewed the torn coat quite neatly.\n[168] │ &lt;He&gt; knew the skill of the great young actress.\n[179] │ &lt;They&gt; felt gay when the ship arrived in port.\n[187] │ &lt;We&gt; admire and love a good cook.\n[189] │ &lt;He&gt; carved a head from the round block of marble.\n[190] │ &lt;She&gt; has a smart way of wearing clothes.\n[199] │ &lt;They&gt; could laugh although they were sad.\n[209] │ &lt;He&gt; wrote his last novel there at the inn.\n[225] │ &lt;They&gt; took the axe and the saw to the forest.\n[232] │ &lt;They&gt; are pushed back each time they attack.\n[233] │ &lt;He&gt; broke his ties with groups of former friends.\n... and 37 more\n\nstr_view(sentences, \"^(She|He|We|I|You|They)\\\\b\")\n\n [61] │ &lt;We&gt; talked of the side show in the circus.\n [63] │ &lt;He&gt; ran half way to the hardware store.\n [90] │ &lt;He&gt; lay prone and hardly moved a limb.\n[116] │ &lt;He&gt; ordered peach pie with ice cream.\n[120] │ &lt;We&gt; find joy in the simplest things.\n[132] │ &lt;He&gt; said the same phrase thirty times.\n[139] │ &lt;We&gt; frown when events take a bad turn.\n[153] │ &lt;He&gt; broke a new shoelace that day.\n[158] │ &lt;We&gt; tried to replace the coin but failed.\n[159] │ &lt;She&gt; sewed the torn coat quite neatly.\n[168] │ &lt;He&gt; knew the skill of the great young actress.\n[179] │ &lt;They&gt; felt gay when the ship arrived in port.\n[187] │ &lt;We&gt; admire and love a good cook.\n[189] │ &lt;He&gt; carved a head from the round block of marble.\n[190] │ &lt;She&gt; has a smart way of wearing clothes.\n[199] │ &lt;They&gt; could laugh although they were sad.\n[209] │ &lt;He&gt; wrote his last novel there at the inn.\n[225] │ &lt;They&gt; took the axe and the saw to the forest.\n[232] │ &lt;They&gt; are pushed back each time they attack.\n[233] │ &lt;He&gt; broke his ties with groups of former friends.\n... and 37 more\n\n\nTo make sure the regex works and to spot errors, create some positvive and negative matches and test them to test that patterns work as expected:\n\npos &lt;- c(\"He is a boy\", \"She had a good time\")\nneg &lt;- c(\"Shells come from the sea\", \"Hadley said 'It's a great day'\")\n\npattern &lt;- \"^(She|He|It|They)\\\\b\"\nstr_detect(pos, pattern)\n\n[1] TRUE TRUE\n\nstr_detect(neg, pattern)\n\n[1] FALSE FALSE\n\n\n\nBoolean operations\n\nWhen you want to find words that only contain consonants, you can create a character class that contains all letters except for the vowels ([^aeiou]), then allow that to match any number of letters ([^aeiou]+), then force it to match the whole string by anchoring to the beginning and the end (^[^aeiou]+$).\n\nstr_view(words, \"^[^aeiou]+$\")\n\n[123] │ &lt;by&gt;\n[249] │ &lt;dry&gt;\n[328] │ &lt;fly&gt;\n[538] │ &lt;mrs&gt;\n[895] │ &lt;try&gt;\n[952] │ &lt;why&gt;\n\n\nAnother alternative is, instead of matching consonants, one can match words that do not contain vowels.\n\nwords[!str_detect(words, \"[aeiou]\")]\n\n[1] \"by\"  \"dry\" \"fly\" \"mrs\" \"try\" \"why\"\n\n\n\n\n\n\n\n\nTipImportant\n\n\n\nWhenever you are dealing with logical combinations, particularly those involving “and” or “or”, or in general when you get stuck trying to create a single regex to solve your problem, **step back and think if you can break the problem into smaller pieces, solving each challenge before moving onto the next one, and combine them in several str_detect() calls.\n\n\nTo fin a words that contain “a” and “b”, you match words that contain “a” followed by “b” or “b” followed by “a” using str_view(words, \"a.*b|b.*a\") or use a simpler approach of two calls to str_detect() such as str_detect(words, \"a\") & str_detect(words, \"b\").\nWhat if we wanted to see if there was a word that contains all vowels? If we did it with patterns we’d need to generate 5! (120) different patterns:\nwords[str_detect(words, \"a.*e.*i.*o.*u\")]\n# ...\nwords[str_detect(words, \"u.*o.*i.*e.*a\")]\nBut it is much simpler to combine five calls to st_detect():\nwords[\n  str_detect(words, \"a\") &\n  str_detect(words, \"e\") &\n  str_detect(words, \"i\") &\n  str_detect(words, \"o\") &\n  str_detect(words, \"u\")\n]\n\nCreating a pattern with code\n\nIf we want to find all sentences that mention a color, we can:\n\nstr_view(sentences, \"\\\\b(red|green|blue)\\\\b\")\n\n  [2] │ Glue the sheet to the dark &lt;blue&gt; background.\n [26] │ Two &lt;blue&gt; fish swam in the tank.\n [92] │ A wisp of cloud hung in the &lt;blue&gt; air.\n[148] │ The spot on the blotter was made by &lt;green&gt; ink.\n[160] │ The sofa cushion is &lt;red&gt; and of light weight.\n[174] │ The sky that morning was clear and bright &lt;blue&gt;.\n[204] │ A &lt;blue&gt; crane is a tall wading bird.\n[217] │ It is hard to erase &lt;blue&gt; or &lt;red&gt; ink.\n[224] │ The lamp shone with a steady &lt;green&gt; flame.\n[247] │ The box is held by a bright &lt;red&gt; snapper.\n[256] │ The houses are built of &lt;red&gt; clay bricks.\n[274] │ The &lt;red&gt; tape bound the smuggled food.\n[288] │ Hedge apples may stain your hands &lt;green&gt;.\n[302] │ The plant grew large and &lt;green&gt; in the window.\n[330] │ Bathe and relax in the cool &lt;green&gt; grass.\n[368] │ The lake sparkled in the &lt;red&gt; hot sun.\n[372] │ Mark the spot with a sign painted &lt;red&gt;.\n[452] │ The couch cover and hall drapes were &lt;blue&gt;.\n[491] │ A man in a &lt;blue&gt; sweater sat at the desk.\n[551] │ The small &lt;red&gt; neon lamp went out.\n... and 6 more\n\n\nBut writing this gets combersome if is the list of colors is long. We can instead put the colors in a vector vector and use str_c() or/andstr_flatten()` to create the pattern.\n\nrgb &lt;- c(\"red\", \"green\", \"blue\")\n\npattern &lt;- str_c(\"\\\\b(\", str_flatten(rgb, \"|\"), \")\\\\b\")\n\nLet’s we want to use the colors that come with R in colors() function.\nstr_view(colors())\n\ncols &lt;- colors()\n1cols &lt;- cols[!str_detect(cols, \"\\\\d\")]\n2pattern &lt;- str_c(\"\\\\b(\", str_flatten(cols, \"|\"), \")\\\\b\")\n3str_view(sentences, pattern)\n\n1\n\nexclude color names with digitis.\n\n2\n\nbuild the pattern\n\n3\n\napply the pattern and find the match.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhenever you create patterns from existing strings, it’s wise to run them through str_escape() to ensure they match literally.\n\n\n\n\n2.4.19 Execises\n\nQuestion 1Solution\n\n\nFor each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls.\n\nFind all words that start or end with x.\nFind all words that start with a vowel and end with a consonant.\nAre there any words that contain at least one of each different vowel?\n\n\n\n\n#a.\nstr_view(words, \"^x.*|.*x$\") \n\n[108] │ &lt;box&gt;\n[747] │ &lt;sex&gt;\n[772] │ &lt;six&gt;\n[841] │ &lt;tax&gt;\n\nwords[str_detect(words, \"^x\") | str_detect(words, \"x$\")]\n\n[1] \"box\" \"sex\" \"six\" \"tax\"\n\n#b.\nstr_view(words, \"^[aeiou].+[^aeiou]$\")\n\n [3] │ &lt;about&gt;\n [5] │ &lt;accept&gt;\n [6] │ &lt;account&gt;\n [8] │ &lt;across&gt;\n [9] │ &lt;act&gt;\n[11] │ &lt;actual&gt;\n[12] │ &lt;add&gt;\n[13] │ &lt;address&gt;\n[14] │ &lt;admit&gt;\n[16] │ &lt;affect&gt;\n[17] │ &lt;afford&gt;\n[18] │ &lt;after&gt;\n[19] │ &lt;afternoon&gt;\n[20] │ &lt;again&gt;\n[21] │ &lt;against&gt;\n[23] │ &lt;agent&gt;\n[26] │ &lt;air&gt;\n[27] │ &lt;all&gt;\n[28] │ &lt;allow&gt;\n[29] │ &lt;almost&gt;\n... and 93 more\n\nwords[str_detect(words, \"^[aeiou]\") & !str_detect(words, \"[aeiou]$\")]\n\n  [1] \"about\"       \"accept\"      \"account\"     \"across\"      \"act\"        \n  [6] \"actual\"      \"add\"         \"address\"     \"admit\"       \"affect\"     \n [11] \"afford\"      \"after\"       \"afternoon\"   \"again\"       \"against\"    \n [16] \"agent\"       \"air\"         \"all\"         \"allow\"       \"almost\"     \n [21] \"along\"       \"already\"     \"alright\"     \"although\"    \"always\"     \n [26] \"amount\"      \"and\"         \"another\"     \"answer\"      \"any\"        \n [31] \"apart\"       \"apparent\"    \"appear\"      \"apply\"       \"appoint\"    \n [36] \"approach\"    \"arm\"         \"around\"      \"art\"         \"as\"         \n [41] \"ask\"         \"at\"          \"attend\"      \"authority\"   \"away\"       \n [46] \"awful\"       \"each\"        \"early\"       \"east\"        \"easy\"       \n [51] \"eat\"         \"economy\"     \"effect\"      \"egg\"         \"eight\"      \n [56] \"either\"      \"elect\"       \"electric\"    \"eleven\"      \"employ\"     \n [61] \"end\"         \"english\"     \"enjoy\"       \"enough\"      \"enter\"      \n [66] \"environment\" \"equal\"       \"especial\"    \"even\"        \"evening\"    \n [71] \"ever\"        \"every\"       \"exact\"       \"except\"      \"exist\"      \n [76] \"expect\"      \"explain\"     \"express\"     \"identify\"    \"if\"         \n [81] \"important\"   \"in\"          \"indeed\"      \"individual\"  \"industry\"   \n [86] \"inform\"      \"instead\"     \"interest\"    \"invest\"      \"it\"         \n [91] \"item\"        \"obvious\"     \"occasion\"    \"odd\"         \"of\"         \n [96] \"off\"         \"offer\"       \"often\"       \"okay\"        \"old\"        \n[101] \"on\"          \"only\"        \"open\"        \"opportunity\" \"or\"         \n[106] \"order\"       \"original\"    \"other\"       \"ought\"       \"out\"        \n[111] \"over\"        \"own\"         \"under\"       \"understand\"  \"union\"      \n[116] \"unit\"        \"university\"  \"unless\"      \"until\"       \"up\"         \n[121] \"upon\"        \"usual\"      \n\n#c.\nstr_view(words, \"[aeiou]+\")\n\n [1] │ &lt;a&gt;\n [2] │ &lt;a&gt;bl&lt;e&gt;\n [3] │ &lt;a&gt;b&lt;ou&gt;t\n [4] │ &lt;a&gt;bs&lt;o&gt;l&lt;u&gt;t&lt;e&gt;\n [5] │ &lt;a&gt;cc&lt;e&gt;pt\n [6] │ &lt;a&gt;cc&lt;ou&gt;nt\n [7] │ &lt;a&gt;ch&lt;ie&gt;v&lt;e&gt;\n [8] │ &lt;a&gt;cr&lt;o&gt;ss\n [9] │ &lt;a&gt;ct\n[10] │ &lt;a&gt;ct&lt;i&gt;v&lt;e&gt;\n[11] │ &lt;a&gt;ct&lt;ua&gt;l\n[12] │ &lt;a&gt;dd\n[13] │ &lt;a&gt;ddr&lt;e&gt;ss\n[14] │ &lt;a&gt;dm&lt;i&gt;t\n[15] │ &lt;a&gt;dv&lt;e&gt;rt&lt;i&gt;s&lt;e&gt;\n[16] │ &lt;a&gt;ff&lt;e&gt;ct\n[17] │ &lt;a&gt;ff&lt;o&gt;rd\n[18] │ &lt;a&gt;ft&lt;e&gt;r\n[19] │ &lt;a&gt;ft&lt;e&gt;rn&lt;oo&gt;n\n[20] │ &lt;a&gt;g&lt;ai&gt;n\n... and 954 more\n\nwords[str_detect(words, \"[aeiou]\")]\n\n  [1] \"a\"           \"able\"        \"about\"       \"absolute\"    \"accept\"     \n  [6] \"account\"     \"achieve\"     \"across\"      \"act\"         \"active\"     \n [11] \"actual\"      \"add\"         \"address\"     \"admit\"       \"advertise\"  \n [16] \"affect\"      \"afford\"      \"after\"       \"afternoon\"   \"again\"      \n [21] \"against\"     \"age\"         \"agent\"       \"ago\"         \"agree\"      \n [26] \"air\"         \"all\"         \"allow\"       \"almost\"      \"along\"      \n [31] \"already\"     \"alright\"     \"also\"        \"although\"    \"always\"     \n [36] \"america\"     \"amount\"      \"and\"         \"another\"     \"answer\"     \n [41] \"any\"         \"apart\"       \"apparent\"    \"appear\"      \"apply\"      \n [46] \"appoint\"     \"approach\"    \"appropriate\" \"area\"        \"argue\"      \n [51] \"arm\"         \"around\"      \"arrange\"     \"art\"         \"as\"         \n [56] \"ask\"         \"associate\"   \"assume\"      \"at\"          \"attend\"     \n [61] \"authority\"   \"available\"   \"aware\"       \"away\"        \"awful\"      \n [66] \"baby\"        \"back\"        \"bad\"         \"bag\"         \"balance\"    \n [71] \"ball\"        \"bank\"        \"bar\"         \"base\"        \"basis\"      \n [76] \"be\"          \"bear\"        \"beat\"        \"beauty\"      \"because\"    \n [81] \"become\"      \"bed\"         \"before\"      \"begin\"       \"behind\"     \n [86] \"believe\"     \"benefit\"     \"best\"        \"bet\"         \"between\"    \n [91] \"big\"         \"bill\"        \"birth\"       \"bit\"         \"black\"      \n [96] \"bloke\"       \"blood\"       \"blow\"        \"blue\"        \"board\"      \n[101] \"boat\"        \"body\"        \"book\"        \"both\"        \"bother\"     \n[106] \"bottle\"      \"bottom\"      \"box\"         \"boy\"         \"break\"      \n[111] \"brief\"       \"brilliant\"   \"bring\"       \"britain\"     \"brother\"    \n[116] \"budget\"      \"build\"       \"bus\"         \"business\"    \"busy\"       \n[121] \"but\"         \"buy\"         \"cake\"        \"call\"        \"can\"        \n[126] \"car\"         \"card\"        \"care\"        \"carry\"       \"case\"       \n[131] \"cat\"         \"catch\"       \"cause\"       \"cent\"        \"centre\"     \n[136] \"certain\"     \"chair\"       \"chairman\"    \"chance\"      \"change\"     \n[141] \"chap\"        \"character\"   \"charge\"      \"cheap\"       \"check\"      \n[146] \"child\"       \"choice\"      \"choose\"      \"Christ\"      \"Christmas\"  \n[151] \"church\"      \"city\"        \"claim\"       \"class\"       \"clean\"      \n[156] \"clear\"       \"client\"      \"clock\"       \"close\"       \"closes\"     \n[161] \"clothe\"      \"club\"        \"coffee\"      \"cold\"        \"colleague\"  \n[166] \"collect\"     \"college\"     \"colour\"      \"come\"        \"comment\"    \n[171] \"commit\"      \"committee\"   \"common\"      \"community\"   \"company\"    \n[176] \"compare\"     \"complete\"    \"compute\"     \"concern\"     \"condition\"  \n[181] \"confer\"      \"consider\"    \"consult\"     \"contact\"     \"continue\"   \n[186] \"contract\"    \"control\"     \"converse\"    \"cook\"        \"copy\"       \n[191] \"corner\"      \"correct\"     \"cost\"        \"could\"       \"council\"    \n[196] \"count\"       \"country\"     \"county\"      \"couple\"      \"course\"     \n[201] \"court\"       \"cover\"       \"create\"      \"cross\"       \"cup\"        \n[206] \"current\"     \"cut\"         \"dad\"         \"danger\"      \"date\"       \n[211] \"day\"         \"dead\"        \"deal\"        \"dear\"        \"debate\"     \n[216] \"decide\"      \"decision\"    \"deep\"        \"definite\"    \"degree\"     \n[221] \"department\"  \"depend\"      \"describe\"    \"design\"      \"detail\"     \n[226] \"develop\"     \"die\"         \"difference\"  \"difficult\"   \"dinner\"     \n[231] \"direct\"      \"discuss\"     \"district\"    \"divide\"      \"do\"         \n[236] \"doctor\"      \"document\"    \"dog\"         \"door\"        \"double\"     \n[241] \"doubt\"       \"down\"        \"draw\"        \"dress\"       \"drink\"      \n[246] \"drive\"       \"drop\"        \"due\"         \"during\"      \"each\"       \n[251] \"early\"       \"east\"        \"easy\"        \"eat\"         \"economy\"    \n[256] \"educate\"     \"effect\"      \"egg\"         \"eight\"       \"either\"     \n[261] \"elect\"       \"electric\"    \"eleven\"      \"else\"        \"employ\"     \n[266] \"encourage\"   \"end\"         \"engine\"      \"english\"     \"enjoy\"      \n[271] \"enough\"      \"enter\"       \"environment\" \"equal\"       \"especial\"   \n[276] \"europe\"      \"even\"        \"evening\"     \"ever\"        \"every\"      \n[281] \"evidence\"    \"exact\"       \"example\"     \"except\"      \"excuse\"     \n[286] \"exercise\"    \"exist\"       \"expect\"      \"expense\"     \"experience\" \n[291] \"explain\"     \"express\"     \"extra\"       \"eye\"         \"face\"       \n[296] \"fact\"        \"fair\"        \"fall\"        \"family\"      \"far\"        \n[301] \"farm\"        \"fast\"        \"father\"      \"favour\"      \"feed\"       \n[306] \"feel\"        \"few\"         \"field\"       \"fight\"       \"figure\"     \n[311] \"file\"        \"fill\"        \"film\"        \"final\"       \"finance\"    \n[316] \"find\"        \"fine\"        \"finish\"      \"fire\"        \"first\"      \n[321] \"fish\"        \"fit\"         \"five\"        \"flat\"        \"floor\"      \n[326] \"follow\"      \"food\"        \"foot\"        \"for\"         \"force\"      \n[331] \"forget\"      \"form\"        \"fortune\"     \"forward\"     \"four\"       \n[336] \"france\"      \"free\"        \"friday\"      \"friend\"      \"from\"       \n[341] \"front\"       \"full\"        \"fun\"         \"function\"    \"fund\"       \n[346] \"further\"     \"future\"      \"game\"        \"garden\"      \"gas\"        \n[351] \"general\"     \"germany\"     \"get\"         \"girl\"        \"give\"       \n[356] \"glass\"       \"go\"          \"god\"         \"good\"        \"goodbye\"    \n[361] \"govern\"      \"grand\"       \"grant\"       \"great\"       \"green\"      \n[366] \"ground\"      \"group\"       \"grow\"        \"guess\"       \"guy\"        \n[371] \"hair\"        \"half\"        \"hall\"        \"hand\"        \"hang\"       \n[376] \"happen\"      \"happy\"       \"hard\"        \"hate\"        \"have\"       \n[381] \"he\"          \"head\"        \"health\"      \"hear\"        \"heart\"      \n[386] \"heat\"        \"heavy\"       \"hell\"        \"help\"        \"here\"       \n[391] \"high\"        \"history\"     \"hit\"         \"hold\"        \"holiday\"    \n[396] \"home\"        \"honest\"      \"hope\"        \"horse\"       \"hospital\"   \n[401] \"hot\"         \"hour\"        \"house\"       \"how\"         \"however\"    \n[406] \"hullo\"       \"hundred\"     \"husband\"     \"idea\"        \"identify\"   \n[411] \"if\"          \"imagine\"     \"important\"   \"improve\"     \"in\"         \n[416] \"include\"     \"income\"      \"increase\"    \"indeed\"      \"individual\" \n[421] \"industry\"    \"inform\"      \"inside\"      \"instead\"     \"insure\"     \n[426] \"interest\"    \"into\"        \"introduce\"   \"invest\"      \"involve\"    \n[431] \"issue\"       \"it\"          \"item\"        \"jesus\"       \"job\"        \n[436] \"join\"        \"judge\"       \"jump\"        \"just\"        \"keep\"       \n[441] \"key\"         \"kid\"         \"kill\"        \"kind\"        \"king\"       \n[446] \"kitchen\"     \"knock\"       \"know\"        \"labour\"      \"lad\"        \n[451] \"lady\"        \"land\"        \"language\"    \"large\"       \"last\"       \n[456] \"late\"        \"laugh\"       \"law\"         \"lay\"         \"lead\"       \n[461] \"learn\"       \"leave\"       \"left\"        \"leg\"         \"less\"       \n[466] \"let\"         \"letter\"      \"level\"       \"lie\"         \"life\"       \n[471] \"light\"       \"like\"        \"likely\"      \"limit\"       \"line\"       \n[476] \"link\"        \"list\"        \"listen\"      \"little\"      \"live\"       \n[481] \"load\"        \"local\"       \"lock\"        \"london\"      \"long\"       \n[486] \"look\"        \"lord\"        \"lose\"        \"lot\"         \"love\"       \n[491] \"low\"         \"luck\"        \"lunch\"       \"machine\"     \"main\"       \n[496] \"major\"       \"make\"        \"man\"         \"manage\"      \"many\"       \n[501] \"mark\"        \"market\"      \"marry\"       \"match\"       \"matter\"     \n[506] \"may\"         \"maybe\"       \"mean\"        \"meaning\"     \"measure\"    \n[511] \"meet\"        \"member\"      \"mention\"     \"middle\"      \"might\"      \n[516] \"mile\"        \"milk\"        \"million\"     \"mind\"        \"minister\"   \n[521] \"minus\"       \"minute\"      \"miss\"        \"mister\"      \"moment\"     \n[526] \"monday\"      \"money\"       \"month\"       \"more\"        \"morning\"    \n[531] \"most\"        \"mother\"      \"motion\"      \"move\"        \"much\"       \n[536] \"music\"       \"must\"        \"name\"        \"nation\"      \"nature\"     \n[541] \"near\"        \"necessary\"   \"need\"        \"never\"       \"new\"        \n[546] \"news\"        \"next\"        \"nice\"        \"night\"       \"nine\"       \n[551] \"no\"          \"non\"         \"none\"        \"normal\"      \"north\"      \n[556] \"not\"         \"note\"        \"notice\"      \"now\"         \"number\"     \n[561] \"obvious\"     \"occasion\"    \"odd\"         \"of\"          \"off\"        \n[566] \"offer\"       \"office\"      \"often\"       \"okay\"        \"old\"        \n[571] \"on\"          \"once\"        \"one\"         \"only\"        \"open\"       \n[576] \"operate\"     \"opportunity\" \"oppose\"      \"or\"          \"order\"      \n[581] \"organize\"    \"original\"    \"other\"       \"otherwise\"   \"ought\"      \n[586] \"out\"         \"over\"        \"own\"         \"pack\"        \"page\"       \n[591] \"paint\"       \"pair\"        \"paper\"       \"paragraph\"   \"pardon\"     \n[596] \"parent\"      \"park\"        \"part\"        \"particular\"  \"party\"      \n[601] \"pass\"        \"past\"        \"pay\"         \"pence\"       \"pension\"    \n[606] \"people\"      \"per\"         \"percent\"     \"perfect\"     \"perhaps\"    \n[611] \"period\"      \"person\"      \"photograph\"  \"pick\"        \"picture\"    \n[616] \"piece\"       \"place\"       \"plan\"        \"play\"        \"please\"     \n[621] \"plus\"        \"point\"       \"police\"      \"policy\"      \"politic\"    \n[626] \"poor\"        \"position\"    \"positive\"    \"possible\"    \"post\"       \n[631] \"pound\"       \"power\"       \"practise\"    \"prepare\"     \"present\"    \n[636] \"press\"       \"pressure\"    \"presume\"     \"pretty\"      \"previous\"   \n[641] \"price\"       \"print\"       \"private\"     \"probable\"    \"problem\"    \n[646] \"proceed\"     \"process\"     \"produce\"     \"product\"     \"programme\"  \n[651] \"project\"     \"proper\"      \"propose\"     \"protect\"     \"provide\"    \n[656] \"public\"      \"pull\"        \"purpose\"     \"push\"        \"put\"        \n[661] \"quality\"     \"quarter\"     \"question\"    \"quick\"       \"quid\"       \n[666] \"quiet\"       \"quite\"       \"radio\"       \"rail\"        \"raise\"      \n[671] \"range\"       \"rate\"        \"rather\"      \"read\"        \"ready\"      \n[676] \"real\"        \"realise\"     \"really\"      \"reason\"      \"receive\"    \n[681] \"recent\"      \"reckon\"      \"recognize\"   \"recommend\"   \"record\"     \n[686] \"red\"         \"reduce\"      \"refer\"       \"regard\"      \"region\"     \n[691] \"relation\"    \"remember\"    \"report\"      \"represent\"   \"require\"    \n[696] \"research\"    \"resource\"    \"respect\"     \"responsible\" \"rest\"       \n[701] \"result\"      \"return\"      \"rid\"         \"right\"       \"ring\"       \n[706] \"rise\"        \"road\"        \"role\"        \"roll\"        \"room\"       \n[711] \"round\"       \"rule\"        \"run\"         \"safe\"        \"sale\"       \n[716] \"same\"        \"saturday\"    \"save\"        \"say\"         \"scheme\"     \n[721] \"school\"      \"science\"     \"score\"       \"scotland\"    \"seat\"       \n[726] \"second\"      \"secretary\"   \"section\"     \"secure\"      \"see\"        \n[731] \"seem\"        \"self\"        \"sell\"        \"send\"        \"sense\"      \n[736] \"separate\"    \"serious\"     \"serve\"       \"service\"     \"set\"        \n[741] \"settle\"      \"seven\"       \"sex\"         \"shall\"       \"share\"      \n[746] \"she\"         \"sheet\"       \"shoe\"        \"shoot\"       \"shop\"       \n[751] \"short\"       \"should\"      \"show\"        \"shut\"        \"sick\"       \n[756] \"side\"        \"sign\"        \"similar\"     \"simple\"      \"since\"      \n[761] \"sing\"        \"single\"      \"sir\"         \"sister\"      \"sit\"        \n[766] \"site\"        \"situate\"     \"six\"         \"size\"        \"sleep\"      \n[771] \"slight\"      \"slow\"        \"small\"       \"smoke\"       \"so\"         \n[776] \"social\"      \"society\"     \"some\"        \"son\"         \"soon\"       \n[781] \"sorry\"       \"sort\"        \"sound\"       \"south\"       \"space\"      \n[786] \"speak\"       \"special\"     \"specific\"    \"speed\"       \"spell\"      \n[791] \"spend\"       \"square\"      \"staff\"       \"stage\"       \"stairs\"     \n[796] \"stand\"       \"standard\"    \"start\"       \"state\"       \"station\"    \n[801] \"stay\"        \"step\"        \"stick\"       \"still\"       \"stop\"       \n[806] \"story\"       \"straight\"    \"strategy\"    \"street\"      \"strike\"     \n[811] \"strong\"      \"structure\"   \"student\"     \"study\"       \"stuff\"      \n[816] \"stupid\"      \"subject\"     \"succeed\"     \"such\"        \"sudden\"     \n[821] \"suggest\"     \"suit\"        \"summer\"      \"sun\"         \"sunday\"     \n[826] \"supply\"      \"support\"     \"suppose\"     \"sure\"        \"surprise\"   \n[831] \"switch\"      \"system\"      \"table\"       \"take\"        \"talk\"       \n[836] \"tape\"        \"tax\"         \"tea\"         \"teach\"       \"team\"       \n[841] \"telephone\"   \"television\"  \"tell\"        \"ten\"         \"tend\"       \n[846] \"term\"        \"terrible\"    \"test\"        \"than\"        \"thank\"      \n[851] \"the\"         \"then\"        \"there\"       \"therefore\"   \"they\"       \n[856] \"thing\"       \"think\"       \"thirteen\"    \"thirty\"      \"this\"       \n[861] \"thou\"        \"though\"      \"thousand\"    \"three\"       \"through\"    \n[866] \"throw\"       \"thursday\"    \"tie\"         \"time\"        \"to\"         \n[871] \"today\"       \"together\"    \"tomorrow\"    \"tonight\"     \"too\"        \n[876] \"top\"         \"total\"       \"touch\"       \"toward\"      \"town\"       \n[881] \"trade\"       \"traffic\"     \"train\"       \"transport\"   \"travel\"     \n[886] \"treat\"       \"tree\"        \"trouble\"     \"true\"        \"trust\"      \n[891] \"tuesday\"     \"turn\"        \"twelve\"      \"twenty\"      \"two\"        \n[896] \"type\"        \"under\"       \"understand\"  \"union\"       \"unit\"       \n[901] \"unite\"       \"university\"  \"unless\"      \"until\"       \"up\"         \n[906] \"upon\"        \"use\"         \"usual\"       \"value\"       \"various\"    \n[911] \"very\"        \"video\"       \"view\"        \"village\"     \"visit\"      \n[916] \"vote\"        \"wage\"        \"wait\"        \"walk\"        \"wall\"       \n[921] \"want\"        \"war\"         \"warm\"        \"wash\"        \"waste\"      \n[926] \"watch\"       \"water\"       \"way\"         \"we\"          \"wear\"       \n[931] \"wednesday\"   \"wee\"         \"week\"        \"weigh\"       \"welcome\"    \n[936] \"well\"        \"west\"        \"what\"        \"when\"        \"where\"      \n[941] \"whether\"     \"which\"       \"while\"       \"white\"       \"who\"        \n[946] \"whole\"       \"wide\"        \"wife\"        \"will\"        \"win\"        \n[951] \"wind\"        \"window\"      \"wish\"        \"with\"        \"within\"     \n[956] \"without\"     \"woman\"       \"wonder\"      \"wood\"        \"word\"       \n[961] \"work\"        \"world\"       \"worry\"       \"worse\"       \"worth\"      \n[966] \"would\"       \"write\"       \"wrong\"       \"year\"        \"yes\"        \n[971] \"yesterday\"   \"yet\"         \"you\"         \"young\"      \n\n\n\n\n\n\nQuestion 2\n\n\nConstruct patterns to find evidence for and against the rule “i before e except after c”?\n\n\n\n\nQuestion 3Solution\n\n\ncolors() contains a number of modifiers like “lightgray” and “darkblue”. How could you automatically identify these modifiers? (Think about how you might detect and then remove the colors that are modified).\n\n\n\ncolrs &lt;- colors()\ncolrs[!str_detect(colrs, \"\\\\d\")]\n\n  [1] \"white\"                \"aliceblue\"            \"antiquewhite\"        \n  [4] \"aquamarine\"           \"azure\"                \"beige\"               \n  [7] \"bisque\"               \"black\"                \"blanchedalmond\"      \n [10] \"blue\"                 \"blueviolet\"           \"brown\"               \n [13] \"burlywood\"            \"cadetblue\"            \"chartreuse\"          \n [16] \"chocolate\"            \"coral\"                \"cornflowerblue\"      \n [19] \"cornsilk\"             \"cyan\"                 \"darkblue\"            \n [22] \"darkcyan\"             \"darkgoldenrod\"        \"darkgray\"            \n [25] \"darkgreen\"            \"darkgrey\"             \"darkkhaki\"           \n [28] \"darkmagenta\"          \"darkolivegreen\"       \"darkorange\"          \n [31] \"darkorchid\"           \"darkred\"              \"darksalmon\"          \n [34] \"darkseagreen\"         \"darkslateblue\"        \"darkslategray\"       \n [37] \"darkslategrey\"        \"darkturquoise\"        \"darkviolet\"          \n [40] \"deeppink\"             \"deepskyblue\"          \"dimgray\"             \n [43] \"dimgrey\"              \"dodgerblue\"           \"firebrick\"           \n [46] \"floralwhite\"          \"forestgreen\"          \"gainsboro\"           \n [49] \"ghostwhite\"           \"gold\"                 \"goldenrod\"           \n [52] \"gray\"                 \"green\"                \"greenyellow\"         \n [55] \"grey\"                 \"honeydew\"             \"hotpink\"             \n [58] \"indianred\"            \"ivory\"                \"khaki\"               \n [61] \"lavender\"             \"lavenderblush\"        \"lawngreen\"           \n [64] \"lemonchiffon\"         \"lightblue\"            \"lightcoral\"          \n [67] \"lightcyan\"            \"lightgoldenrod\"       \"lightgoldenrodyellow\"\n [70] \"lightgray\"            \"lightgreen\"           \"lightgrey\"           \n [73] \"lightpink\"            \"lightsalmon\"          \"lightseagreen\"       \n [76] \"lightskyblue\"         \"lightslateblue\"       \"lightslategray\"      \n [79] \"lightslategrey\"       \"lightsteelblue\"       \"lightyellow\"         \n [82] \"limegreen\"            \"linen\"                \"magenta\"             \n [85] \"maroon\"               \"mediumaquamarine\"     \"mediumblue\"          \n [88] \"mediumorchid\"         \"mediumpurple\"         \"mediumseagreen\"      \n [91] \"mediumslateblue\"      \"mediumspringgreen\"    \"mediumturquoise\"     \n [94] \"mediumvioletred\"      \"midnightblue\"         \"mintcream\"           \n [97] \"mistyrose\"            \"moccasin\"             \"navajowhite\"         \n[100] \"navy\"                 \"navyblue\"             \"oldlace\"             \n[103] \"olivedrab\"            \"orange\"               \"orangered\"           \n[106] \"orchid\"               \"palegoldenrod\"        \"palegreen\"           \n[109] \"paleturquoise\"        \"palevioletred\"        \"papayawhip\"          \n[112] \"peachpuff\"            \"peru\"                 \"pink\"                \n[115] \"plum\"                 \"powderblue\"           \"purple\"              \n[118] \"red\"                  \"rosybrown\"            \"royalblue\"           \n[121] \"saddlebrown\"          \"salmon\"               \"sandybrown\"          \n[124] \"seagreen\"             \"seashell\"             \"sienna\"              \n[127] \"skyblue\"              \"slateblue\"            \"slategray\"           \n[130] \"slategrey\"            \"snow\"                 \"springgreen\"         \n[133] \"steelblue\"            \"tan\"                  \"thistle\"             \n[136] \"tomato\"               \"turquoise\"            \"violet\"              \n[139] \"violetred\"            \"wheat\"                \"whitesmoke\"          \n[142] \"yellow\"               \"yellowgreen\"         \n\nrgb &lt;- c(\"red\" , \"green \", \"blue\")\npattern &lt;- str_c(\"(\\\\d+$)|\", \"(\\\\w+)(\", str_flatten(rgb, \"|\"), \")\", \"(\\\\w*)\")\npattern\n\n[1] \"(\\\\d+$)|(\\\\w+)(red|green |blue)(\\\\w*)\"\n\ncolrs[str_detect(colrs, pattern)]\n\n  [1] \"aliceblue\"       \"antiquewhite1\"   \"antiquewhite2\"   \"antiquewhite3\"  \n  [5] \"antiquewhite4\"   \"aquamarine1\"     \"aquamarine2\"     \"aquamarine3\"    \n  [9] \"aquamarine4\"     \"azure1\"          \"azure2\"          \"azure3\"         \n [13] \"azure4\"          \"bisque1\"         \"bisque2\"         \"bisque3\"        \n [17] \"bisque4\"         \"blue1\"           \"blue2\"           \"blue3\"          \n [21] \"blue4\"           \"brown1\"          \"brown2\"          \"brown3\"         \n [25] \"brown4\"          \"burlywood1\"      \"burlywood2\"      \"burlywood3\"     \n [29] \"burlywood4\"      \"cadetblue\"       \"cadetblue1\"      \"cadetblue2\"     \n [33] \"cadetblue3\"      \"cadetblue4\"      \"chartreuse1\"     \"chartreuse2\"    \n [37] \"chartreuse3\"     \"chartreuse4\"     \"chocolate1\"      \"chocolate2\"     \n [41] \"chocolate3\"      \"chocolate4\"      \"coral1\"          \"coral2\"         \n [45] \"coral3\"          \"coral4\"          \"cornflowerblue\"  \"cornsilk1\"      \n [49] \"cornsilk2\"       \"cornsilk3\"       \"cornsilk4\"       \"cyan1\"          \n [53] \"cyan2\"           \"cyan3\"           \"cyan4\"           \"darkblue\"       \n [57] \"darkgoldenrod1\"  \"darkgoldenrod2\"  \"darkgoldenrod3\"  \"darkgoldenrod4\" \n [61] \"darkolivegreen1\" \"darkolivegreen2\" \"darkolivegreen3\" \"darkolivegreen4\"\n [65] \"darkorange1\"     \"darkorange2\"     \"darkorange3\"     \"darkorange4\"    \n [69] \"darkorchid1\"     \"darkorchid2\"     \"darkorchid3\"     \"darkorchid4\"    \n [73] \"darkred\"         \"darkseagreen1\"   \"darkseagreen2\"   \"darkseagreen3\"  \n [77] \"darkseagreen4\"   \"darkslateblue\"   \"darkslategray1\"  \"darkslategray2\" \n [81] \"darkslategray3\"  \"darkslategray4\"  \"deeppink1\"       \"deeppink2\"      \n [85] \"deeppink3\"       \"deeppink4\"       \"deepskyblue\"     \"deepskyblue1\"   \n [89] \"deepskyblue2\"    \"deepskyblue3\"    \"deepskyblue4\"    \"dodgerblue\"     \n [93] \"dodgerblue1\"     \"dodgerblue2\"     \"dodgerblue3\"     \"dodgerblue4\"    \n [97] \"firebrick1\"      \"firebrick2\"      \"firebrick3\"      \"firebrick4\"     \n[101] \"gold1\"           \"gold2\"           \"gold3\"           \"gold4\"          \n[105] \"goldenrod1\"      \"goldenrod2\"      \"goldenrod3\"      \"goldenrod4\"     \n[109] \"gray0\"           \"gray1\"           \"gray2\"           \"gray3\"          \n[113] \"gray4\"           \"gray5\"           \"gray6\"           \"gray7\"          \n[117] \"gray8\"           \"gray9\"           \"gray10\"          \"gray11\"         \n[121] \"gray12\"          \"gray13\"          \"gray14\"          \"gray15\"         \n[125] \"gray16\"          \"gray17\"          \"gray18\"          \"gray19\"         \n[129] \"gray20\"          \"gray21\"          \"gray22\"          \"gray23\"         \n[133] \"gray24\"          \"gray25\"          \"gray26\"          \"gray27\"         \n[137] \"gray28\"          \"gray29\"          \"gray30\"          \"gray31\"         \n[141] \"gray32\"          \"gray33\"          \"gray34\"          \"gray35\"         \n[145] \"gray36\"          \"gray37\"          \"gray38\"          \"gray39\"         \n[149] \"gray40\"          \"gray41\"          \"gray42\"          \"gray43\"         \n[153] \"gray44\"          \"gray45\"          \"gray46\"          \"gray47\"         \n[157] \"gray48\"          \"gray49\"          \"gray50\"          \"gray51\"         \n[161] \"gray52\"          \"gray53\"          \"gray54\"          \"gray55\"         \n[165] \"gray56\"          \"gray57\"          \"gray58\"          \"gray59\"         \n[169] \"gray60\"          \"gray61\"          \"gray62\"          \"gray63\"         \n[173] \"gray64\"          \"gray65\"          \"gray66\"          \"gray67\"         \n[177] \"gray68\"          \"gray69\"          \"gray70\"          \"gray71\"         \n[181] \"gray72\"          \"gray73\"          \"gray74\"          \"gray75\"         \n[185] \"gray76\"          \"gray77\"          \"gray78\"          \"gray79\"         \n[189] \"gray80\"          \"gray81\"          \"gray82\"          \"gray83\"         \n[193] \"gray84\"          \"gray85\"          \"gray86\"          \"gray87\"         \n[197] \"gray88\"          \"gray89\"          \"gray90\"          \"gray91\"         \n[201] \"gray92\"          \"gray93\"          \"gray94\"          \"gray95\"         \n[205] \"gray96\"          \"gray97\"          \"gray98\"          \"gray99\"         \n[209] \"gray100\"         \"green1\"          \"green2\"          \"green3\"         \n[213] \"green4\"          \"grey0\"           \"grey1\"           \"grey2\"          \n[217] \"grey3\"           \"grey4\"           \"grey5\"           \"grey6\"          \n[221] \"grey7\"           \"grey8\"           \"grey9\"           \"grey10\"         \n[225] \"grey11\"          \"grey12\"          \"grey13\"          \"grey14\"         \n[229] \"grey15\"          \"grey16\"          \"grey17\"          \"grey18\"         \n[233] \"grey19\"          \"grey20\"          \"grey21\"          \"grey22\"         \n[237] \"grey23\"          \"grey24\"          \"grey25\"          \"grey26\"         \n[241] \"grey27\"          \"grey28\"          \"grey29\"          \"grey30\"         \n[245] \"grey31\"          \"grey32\"          \"grey33\"          \"grey34\"         \n[249] \"grey35\"          \"grey36\"          \"grey37\"          \"grey38\"         \n[253] \"grey39\"          \"grey40\"          \"grey41\"          \"grey42\"         \n[257] \"grey43\"          \"grey44\"          \"grey45\"          \"grey46\"         \n[261] \"grey47\"          \"grey48\"          \"grey49\"          \"grey50\"         \n[265] \"grey51\"          \"grey52\"          \"grey53\"          \"grey54\"         \n[269] \"grey55\"          \"grey56\"          \"grey57\"          \"grey58\"         \n[273] \"grey59\"          \"grey60\"          \"grey61\"          \"grey62\"         \n[277] \"grey63\"          \"grey64\"          \"grey65\"          \"grey66\"         \n[281] \"grey67\"          \"grey68\"          \"grey69\"          \"grey70\"         \n[285] \"grey71\"          \"grey72\"          \"grey73\"          \"grey74\"         \n[289] \"grey75\"          \"grey76\"          \"grey77\"          \"grey78\"         \n[293] \"grey79\"          \"grey80\"          \"grey81\"          \"grey82\"         \n[297] \"grey83\"          \"grey84\"          \"grey85\"          \"grey86\"         \n[301] \"grey87\"          \"grey88\"          \"grey89\"          \"grey90\"         \n[305] \"grey91\"          \"grey92\"          \"grey93\"          \"grey94\"         \n[309] \"grey95\"          \"grey96\"          \"grey97\"          \"grey98\"         \n[313] \"grey99\"          \"grey100\"         \"honeydew1\"       \"honeydew2\"      \n[317] \"honeydew3\"       \"honeydew4\"       \"hotpink1\"        \"hotpink2\"       \n[321] \"hotpink3\"        \"hotpink4\"        \"indianred\"       \"indianred1\"     \n[325] \"indianred2\"      \"indianred3\"      \"indianred4\"      \"ivory1\"         \n[329] \"ivory2\"          \"ivory3\"          \"ivory4\"          \"khaki1\"         \n[333] \"khaki2\"          \"khaki3\"          \"khaki4\"          \"lavenderblush1\" \n[337] \"lavenderblush2\"  \"lavenderblush3\"  \"lavenderblush4\"  \"lemonchiffon1\"  \n[341] \"lemonchiffon2\"   \"lemonchiffon3\"   \"lemonchiffon4\"   \"lightblue\"      \n[345] \"lightblue1\"      \"lightblue2\"      \"lightblue3\"      \"lightblue4\"     \n[349] \"lightcyan1\"      \"lightcyan2\"      \"lightcyan3\"      \"lightcyan4\"     \n[353] \"lightgoldenrod1\" \"lightgoldenrod2\" \"lightgoldenrod3\" \"lightgoldenrod4\"\n[357] \"lightpink1\"      \"lightpink2\"      \"lightpink3\"      \"lightpink4\"     \n[361] \"lightsalmon1\"    \"lightsalmon2\"    \"lightsalmon3\"    \"lightsalmon4\"   \n[365] \"lightskyblue\"    \"lightskyblue1\"   \"lightskyblue2\"   \"lightskyblue3\"  \n[369] \"lightskyblue4\"   \"lightslateblue\"  \"lightsteelblue\"  \"lightsteelblue1\"\n[373] \"lightsteelblue2\" \"lightsteelblue3\" \"lightsteelblue4\" \"lightyellow1\"   \n[377] \"lightyellow2\"    \"lightyellow3\"    \"lightyellow4\"    \"magenta1\"       \n[381] \"magenta2\"        \"magenta3\"        \"magenta4\"        \"maroon1\"        \n[385] \"maroon2\"         \"maroon3\"         \"maroon4\"         \"mediumblue\"     \n[389] \"mediumorchid1\"   \"mediumorchid2\"   \"mediumorchid3\"   \"mediumorchid4\"  \n[393] \"mediumpurple1\"   \"mediumpurple2\"   \"mediumpurple3\"   \"mediumpurple4\"  \n[397] \"mediumslateblue\" \"mediumvioletred\" \"midnightblue\"    \"mistyrose1\"     \n[401] \"mistyrose2\"      \"mistyrose3\"      \"mistyrose4\"      \"navajowhite1\"   \n[405] \"navajowhite2\"    \"navajowhite3\"    \"navajowhite4\"    \"navyblue\"       \n[409] \"olivedrab1\"      \"olivedrab2\"      \"olivedrab3\"      \"olivedrab4\"     \n[413] \"orange1\"         \"orange2\"         \"orange3\"         \"orange4\"        \n[417] \"orangered\"       \"orangered1\"      \"orangered2\"      \"orangered3\"     \n[421] \"orangered4\"      \"orchid1\"         \"orchid2\"         \"orchid3\"        \n[425] \"orchid4\"         \"palegreen1\"      \"palegreen2\"      \"palegreen3\"     \n[429] \"palegreen4\"      \"paleturquoise1\"  \"paleturquoise2\"  \"paleturquoise3\" \n[433] \"paleturquoise4\"  \"palevioletred\"   \"palevioletred1\"  \"palevioletred2\" \n[437] \"palevioletred3\"  \"palevioletred4\"  \"peachpuff1\"      \"peachpuff2\"     \n[441] \"peachpuff3\"      \"peachpuff4\"      \"pink1\"           \"pink2\"          \n[445] \"pink3\"           \"pink4\"           \"plum1\"           \"plum2\"          \n[449] \"plum3\"           \"plum4\"           \"powderblue\"      \"purple1\"        \n[453] \"purple2\"         \"purple3\"         \"purple4\"         \"red1\"           \n[457] \"red2\"            \"red3\"            \"red4\"            \"rosybrown1\"     \n[461] \"rosybrown2\"      \"rosybrown3\"      \"rosybrown4\"      \"royalblue\"      \n[465] \"royalblue1\"      \"royalblue2\"      \"royalblue3\"      \"royalblue4\"     \n[469] \"salmon1\"         \"salmon2\"         \"salmon3\"         \"salmon4\"        \n[473] \"seagreen1\"       \"seagreen2\"       \"seagreen3\"       \"seagreen4\"      \n[477] \"seashell1\"       \"seashell2\"       \"seashell3\"       \"seashell4\"      \n[481] \"sienna1\"         \"sienna2\"         \"sienna3\"         \"sienna4\"        \n[485] \"skyblue\"         \"skyblue1\"        \"skyblue2\"        \"skyblue3\"       \n[489] \"skyblue4\"        \"slateblue\"       \"slateblue1\"      \"slateblue2\"     \n[493] \"slateblue3\"      \"slateblue4\"      \"slategray1\"      \"slategray2\"     \n[497] \"slategray3\"      \"slategray4\"      \"snow1\"           \"snow2\"          \n[501] \"snow3\"           \"snow4\"           \"springgreen1\"    \"springgreen2\"   \n[505] \"springgreen3\"    \"springgreen4\"    \"steelblue\"       \"steelblue1\"     \n[509] \"steelblue2\"      \"steelblue3\"      \"steelblue4\"      \"tan1\"           \n[513] \"tan2\"            \"tan3\"            \"tan4\"            \"thistle1\"       \n[517] \"thistle2\"        \"thistle3\"        \"thistle4\"        \"tomato1\"        \n[521] \"tomato2\"         \"tomato3\"         \"tomato4\"         \"turquoise1\"     \n[525] \"turquoise2\"      \"turquoise3\"      \"turquoise4\"      \"violetred\"      \n[529] \"violetred1\"      \"violetred2\"      \"violetred3\"      \"violetred4\"     \n[533] \"wheat1\"          \"wheat2\"          \"wheat3\"          \"wheat4\"         \n[537] \"yellow1\"         \"yellow2\"         \"yellow3\"         \"yellow4\"        \n\ncolrs[!str_detect(colrs, pattern)]\n\n  [1] \"white\"                \"antiquewhite\"         \"aquamarine\"          \n  [4] \"azure\"                \"beige\"                \"bisque\"              \n  [7] \"black\"                \"blanchedalmond\"       \"blue\"                \n [10] \"blueviolet\"           \"brown\"                \"burlywood\"           \n [13] \"chartreuse\"           \"chocolate\"            \"coral\"               \n [16] \"cornsilk\"             \"cyan\"                 \"darkcyan\"            \n [19] \"darkgoldenrod\"        \"darkgray\"             \"darkgreen\"           \n [22] \"darkgrey\"             \"darkkhaki\"            \"darkmagenta\"         \n [25] \"darkolivegreen\"       \"darkorange\"           \"darkorchid\"          \n [28] \"darksalmon\"           \"darkseagreen\"         \"darkslategray\"       \n [31] \"darkslategrey\"        \"darkturquoise\"        \"darkviolet\"          \n [34] \"deeppink\"             \"dimgray\"              \"dimgrey\"             \n [37] \"firebrick\"            \"floralwhite\"          \"forestgreen\"         \n [40] \"gainsboro\"            \"ghostwhite\"           \"gold\"                \n [43] \"goldenrod\"            \"gray\"                 \"green\"               \n [46] \"greenyellow\"          \"grey\"                 \"honeydew\"            \n [49] \"hotpink\"              \"ivory\"                \"khaki\"               \n [52] \"lavender\"             \"lavenderblush\"        \"lawngreen\"           \n [55] \"lemonchiffon\"         \"lightcoral\"           \"lightcyan\"           \n [58] \"lightgoldenrod\"       \"lightgoldenrodyellow\" \"lightgray\"           \n [61] \"lightgreen\"           \"lightgrey\"            \"lightpink\"           \n [64] \"lightsalmon\"          \"lightseagreen\"        \"lightslategray\"      \n [67] \"lightslategrey\"       \"lightyellow\"          \"limegreen\"           \n [70] \"linen\"                \"magenta\"              \"maroon\"              \n [73] \"mediumaquamarine\"     \"mediumorchid\"         \"mediumpurple\"        \n [76] \"mediumseagreen\"       \"mediumspringgreen\"    \"mediumturquoise\"     \n [79] \"mintcream\"            \"mistyrose\"            \"moccasin\"            \n [82] \"navajowhite\"          \"navy\"                 \"oldlace\"             \n [85] \"olivedrab\"            \"orange\"               \"orchid\"              \n [88] \"palegoldenrod\"        \"palegreen\"            \"paleturquoise\"       \n [91] \"papayawhip\"           \"peachpuff\"            \"peru\"                \n [94] \"pink\"                 \"plum\"                 \"purple\"              \n [97] \"red\"                  \"rosybrown\"            \"saddlebrown\"         \n[100] \"salmon\"               \"sandybrown\"           \"seagreen\"            \n[103] \"seashell\"             \"sienna\"               \"slategray\"           \n[106] \"slategrey\"            \"snow\"                 \"springgreen\"         \n[109] \"tan\"                  \"thistle\"              \"tomato\"              \n[112] \"turquoise\"            \"violet\"               \"wheat\"               \n[115] \"whitesmoke\"           \"yellow\"               \"yellowgreen\"         \n\n\n\n\n\n\nQuestion 4Solution\n\n\nCreate a regular expression that finds any base R dataset. You can get a list of these datasets via a special use of the data() function: data(package = \"datasets\")$results[, \"Item\"]. Note that a number of old datasets are individual vectors; these contain the name of the grouping “data frame” in parentheses, so you’ll need to strip those off.\n\n\n\nlibrary(tidyverse)\nlibrary(babynames)\ndata()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "04-transform.html#factors",
    "href": "04-transform.html#factors",
    "title": "2  Transform",
    "section": "2.5 Factors",
    "text": "2.5 Factors\nIn addition to base R, we till use the {forcats} package in the core tidyverse.\n\n\n\n\n\n\n\n\n\n2.5.1 Basics\n\n\n\n\n\n\n\n\nfactor() converts missing levels to NA without a warning, which is risky.\nforcats::fct() warns if there is a value missing in the levels: whithout setting levels(), forcats::ftc() orders the levels by appearance.\n\n\n\n\n\n\n\n\nIf you omit the levels, factor() will take them in alphabetical order, but forcats::fct() will order by appearance.\n\n\n\n\n\n\n\n\nYou can also create a factor when reading your data with {webreadr} with col_factor():\n\n\n\n\n\n\n\n\n\n\n2.5.2 General Social Survey (GSS)\nforcats::gss_cat is a sample of data from the GSS, a long-running US survey conducted by the National Opinion Research Center (NORC) at the University of Chicago.\n\n\n\n\n\n\n\n\nWhen factors are stores in a tibble, their levels are not easily visible. count().\n\n\n\n\n\n\n\n\n\n2.5.2.1 Exercises\n\nExplore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot?\n\n\n\n\n\n\n\n\n\n\nWhat is the most common relig in this survey? What’s the most common partyid``\n\n\n\n\n\n\n\n\n\n\nWhich relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualization?\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.3 Modify factor order: fct_reorder() and fct_relevel()\nThis plot presenting the average number of hours spent watching TV by religion (affiliation) is hard to read because the number of hours is not increasing or decreasing.\n\n\n\n\n\n\n\n\nWe may want to modify this plot by changing the order of the levels: We can use fct_reorder() and fct_relevel().\nfct_reorder() takes three arguments: 1. .f, the factor whose levels you want to modify. 2. .x, a numeric vector that you want to use to reorder the levels. 3. Optionally, .fun, a function that’s used if there are multiple values of .x for each value of .f. The default value is median.\n\n\n\n\n\n\n\n\nThe factor-reordering can be done with mutate() before ggplot.\n\n\n\n\n\n\n\n\nPlot how average age varies by income bracket:\n\n\n\n\n\n\n\n\nfct_reorder() is suited for factors with arbitrary order. When factors have a natural ordering, and you want to pull to the front one level, use fct_relevel(). It takes as arguments the factor, .f, and then any number of levels that you want to move to the front of the line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother type of reordering is useful when you are coloring the lines on a plot. fct_reorder2(.f, .x, .y) reorders the factor .f by the .y values associated with the largest .x values. This makes the plot easier to read because the colors of the line at the far right of the plot will line up with the legend.\n\n\n\n\n\n\n\n\nIn bar plots, use fct_infreq() to show order in decreasing frequency: it can be combined with fct_rev() to show order in increasing frequency.\n\n\n\n\n\n\n\n\n\n2.5.3.1 Exercises\n\nThere are some suspiciously high numbers in tvhours. Is the mean a good summary?\nFor each factor in gss_cat identify whether the order of the levels is arbitrary or principled.\nWhy did moving “Not applicable” to the front of the levels move it to the bottom of the plot?\n\n\n\n\n2.5.4 Modify values of factor levels: fct_recode()\nfct_recode() takes as argument .f factor, and the levels to recode/rename with “new levels” on the left = “old levels” on the right. It leaves unchanged levels that are not mentioned and warn if your refer to that do not exist as “old levels”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo combine groups, you can assign multiple old levels to the same new level.\nIf you want to collapse a lot of levels, fct_collapse() is a useful variant of fct_recode(). For each new variable, you can provide a vector of old levels:\n\n\n\n\n\n\n\n\nSometimes, one may want to lump small groups together: this can be chieved with fct_lump_*() family of functions.\nfct_lump_lowfreq()lumps the smallest groups into “Other” category, keeping “Other” always the smallest category.\n\n\n\n\n\n\n\n\nfct_lump_lowfreq()can lump groups in a less useful way: fct_lump_n() can be used to specify the exact number of categories you want to keep.\n\n\n\n\n\n\n\n\nSee also fct_lump_min() and fct_lump_prop().\n\n2.5.4.1 Exercises\n\nHow have the proportions of people identifying as Democrat, Republican, and Independent changed over time?\nHow could you collapse rincome into a small set of categories?\nNotice there are 9 groups (excluding other) in the fct_lump example above. Why not 10? (Hint: type ?fct_lump, and find the default for the argument other_level is “Other”.)\n\n\n\n\n2.5.5 Ordered factors: ordered()\nCreated with the ordered() function, ordered factors imply a strict ordering between levels, but don’t specify anything about the magnitude of the differences between the levels. You use ordered factors when you know there the levels are ranked, but there’s no precise numerical ranking.\nOrdered factors uses &lt;symbols between levels when it is printed.\n\n\n\n\n\n\n\n\nIn both base R and the tidyverse, ordered factors behave very similarly to regular factors. There are only two places where you might notice different behavior:\n\nIf you map an ordered factor to color or fill in ggplot2, it will default to scale_color_viridis()/scale_fill_viridis(), a color scale that implies a ranking.\nIf you use an ordered predictor in a linear model, it will use “polynomial contrasts”. These are mildly useful, but you are unlikely to have heard of them unless you have a PhD in Statistics, and even then you probably don’t routinely interpret them. If you want to learn more, we recommend vignette(“contrasts”, package = “faux”) by Lisa DeBruine.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "04-transform.html#dates-and-times",
    "href": "04-transform.html#dates-and-times",
    "title": "2  Transform",
    "section": "2.6 Dates and times",
    "text": "2.6 Dates and times\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAims\n\n\n\n\nTo teach to create date-times from various inputs.\nTo teach how to extract date-times components.\nTo work with time spans.\nHow to work with time zomes.\n\n\n\nThis section willl use the{lubridate} package and nyflights13 dataset in tydiverse package.\n\n2.6.1 Creating date/times\nThree types of date/time data: - date: tibbles prnt &lt;date&gt; - time within a day: tibbles print &lt;time&gt; - date-time (a date plus a time): identifies an instant in time typically to the nearest second. Tibbles print &lt;dttm&gt;. Base R calls thes POSIXct.\nR does not have a native class for storing times: use {hms} package if need be.\n\n\n\n\n\n\nTip\n\n\n\nUse the simplest date type the works for you needs. Date-times are more complicated because they need to handle time zones: Use a date instead if enough for your needs!\n\n\n\nlibrary(tidyverse)\nlibrary(nycflights13)\nlibrary(gt)\ntoday() # current date\n\n[1] \"2026-01-11\"\n\nnow() # current date-time\n\n[1] \"2026-01-11 18:14:33 CET\"\n\n\nFour ways to create a date/time: - when reading in a file with {readr} - from a string - from individual date-time components - from an existing date/time object\n\n2.6.1.1 During import\nIf your CSV file contains an ISO8601, readr will recognize it.\n\n\n\n\n\n\nNoteISO8601\n\n\n\nISO8601 is an international standard for writing dates where the components are organised from biggest to smallest (year to day) separated by -. ISO8601 can includes hour, minute, and second separated by :; and date and time components are separated with a T or a space.\nFor example, 4:326pm on May 2 2022 is 2022-05-03 16:26 or 2022-05-03T1626.\n\n\nFor other date/time formats not in ISO8601, use col_types() plus col_date() or col_datetime(). Table 2.1 shows date formats understood by readr.\n\n\n\n\nTable 2.1: Date formats understood by readr\n\n\n\n\n\n\n\n\n\ncode\ndescription\nexample\n\n\n\n\nYear\n\n\n`%Y-%m-%d`\nYear-month-day (ISO 8601)\n2010-01-01\n\n\n`%Y/%m/%d`\nYear/month/day\n2010/01/01\n\n\n`%Y%m%d`\nCompact year-month-day\n20100101\n\n\n`%y-%m-%d`\nTwo-digit year\n10-01-01\n\n\n`%m/%d/%Y`\nUS style: month/day/year\n01/31/2010\n\n\n`%m-%d-%Y`\nUS style with dashes\n01-31-2010\n\n\n`%d/%m/%Y`\nEuropean style: day/month/year\n31/01/2010\n\n\n`%d-%m-%Y`\nEuropean style with dashes\n31-01-2010\n\n\n`%d %b %Y`\nDay abbreviated-month year\n01 Jan 2010\n\n\n`%d %B %Y`\nDay full-month year\n01 January 2010\n\n\n`%b %d, %Y`\nAbbrev-month day, year\nJan 01, 2010\n\n\n`%B %d, %Y`\nFull-month day, year\nJanuary 01, 2010\n\n\n`%d.%m.%Y`\nDot-separated day.month.year\n31.01.2010\n\n\n`%e %b %Y`\nDay with leading space allowed\n1 Jan 2010\n\n\n`%Y-%j`\nYear and day of year (Julian day)\n2010-032\n\n\nTime\n\n\n`%H:%M:%S`\n24-hour time with seconds\n16:26:00\n\n\n`%H:%M`\n24-hour time without seconds\n16:26\n\n\n`%I:%M %p`\n12-hour time with AM/PM\n04:26 PM\n\n\n`%H%M`\nCompact 24-hour time (no colon)\n1626\n\n\n`%T`\nAlias for %H:%M:%S\n16:26:00\n\n\n`%R`\nAlias for %H:%M\n16:26\n\n\n`%S`\nSecond (00–59)\n00\n\n\n`%H`\nHour (00–23)\n16\n\n\n`%I`\nHour (01–12)\n04\n\n\n`%p`\nAM/PM indicator\nPM\n\n\n`%z`\nTimezone offset from UTC\n+0200\n\n\n`%Z`\nTimezone abbreviation\nCEST\n\n\nOther\n\n\n`%.`\n:\nSkip one digit\n\n\n%*\n\nSkip any number of non-digits",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "04-transform.html#mssing-values",
    "href": "04-transform.html#mssing-values",
    "title": "2  Transform",
    "section": "2.7 Mssing values",
    "text": "2.7 Mssing values",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "04-transform.html#joins",
    "href": "04-transform.html#joins",
    "title": "2  Transform",
    "section": "2.8 Joins",
    "text": "2.8 Joins",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Transform</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]